[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Timeline\n\npraktikum-1: 1 Maret 2023, presensi ristek.link/presensi-sains-data-01\npraktikum-2: 8 Maret 2023, presensi ristek.link/presensi-sains-data-02\npraktikum-3: 15 Maret 2023, presensi ristek.link/presensi-sains-data-03\nTugas-1: 22 Maret 2023, tempat pengumpulan: bit.ly/Tugas1PrakSainsData\nTugas-2: 21 April 2023, tempat pengumpulan: https://ristek.link/tugas-sains-data-02\nTugas-3: 21 April 2023, tempat pengumpulan: https://ristek.link/tugas-sains-data-03\npraktikum-4: 26 April 2023, presensi ristek.link/presensi-sains-data-04"
  },
  {
    "objectID": "main-module/module-tahun-lalu.html",
    "href": "main-module/module-tahun-lalu.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "-Module 2021/2022-\nberikut ini adalah module pengajaran sains-data tahun 2021/2022. https://drive.google.com/open?id=1x2SR_L3pWH0W8Z0IUbL1ifBOcMSkWVYe&authuser=carlesoctavianus%40gmail.com&usp=drive_fs"
  },
  {
    "objectID": "main-module/week-01.html",
    "href": "main-module/week-01.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Pada module ini kita akan coba mememahami package pandas, yang merupakan package inti dalam sains-data. kita akan coba melakukan beberapa transformasi data menggunakan pandas.\nsebelum itu, python module di bawah ini yang akan digunakan selama praktikum.\n\nimport numpy as np\nimport pandas as pd\n\n\n\n\npandas.Series sangat mirip dengan array NumPy (bahkan dibangun di atas objek array NumPy). Yang membedakan array NumPy dari sebuah Series adalah bahwa sebuah Series dapat memiliki label index, yang berarti dapat diindeks dengan label, bukan hanya lokasi nomor saja. Selain itu, sebuah Series tidak perlu menyimpan data numerik, ia dapat menyimpan objek Python sembarang.\n\n\nPaling mudah, ktia dapat membuat pd.Series dengan python list\n\nmy_index= ['a','b','c','d','e']\nmy_data= [1,2,3,4,5]\nmy_series= pd.Series(data=my_data, index=my_index)\n\n\nprint(my_series)\nprint(my_series.__class__)\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\n<class 'pandas.core.series.Series'>\n\n\n\n\n\nKita juga dapat membuat pd.Series dengan dictionary\n\n# creating a series from a dictionary\nmy_dict= {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}\nmy_series_dict= pd.Series(my_dict)\n\n\nprint(my_series_dict)\nprint(my_series_dict.__class__)\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\n<class 'pandas.core.series.Series'>\n\n\n\n\n\n\n# Imaginary Sales Data for 1st and 2nd Quarters for Global Company\nq1 = {'Japan': 80, 'China': 450, 'India': 200, 'USA': 250}\nq2 = {'Brazil': 100,'China': 500, 'India': 210,'USA': 260}\n\n\n# Creating a Series from a Dictionary q1 and q2\nq1_series= pd.Series(q1)\nq2_series= pd.Series(q2)\n\n\nprint(q1_series)\n\nJapan     80\nChina    450\nIndia    200\nUSA      250\ndtype: int64\n\n\nKita dapat mengindeks dengan label\n\n# call values of q1_series based on named index\nprint(q1_series['Japan'])\nprint(q1_series['China'])\nprint(q1_series['India'])\n\n80\n450\n200\n\n\nkita dapat tetap dapat mengindeks dengan integer\n\n# u can also call values of q1_series based on positional index\nprint(q1_series[0])\nprint(q1_series[1])\nprint(q1_series[2])\n\n80\n450\n200\n\n\nhati-hati dalam melakukan indexing dengan label. bisa saja terjadi error jika label tidak ada di dalam pd.series\n\n# remember named index is case sensitive\ntry:\n    print(q1_series['japan'])\nexcept:\n    print('something went wrong')\n\nsomething went wrong\n\n\nOperasi aritmatik sederhana pada pd.Series bersifat broadcasting\n\n# operations with arithmetic on series are broadcasted to all values\nprint(q1_series*2)\n\nJapan    160\nChina    900\nIndia    400\nUSA      500\ndtype: int64\n\n\n\nprint(q1_series+1000)\n\nJapan    1080\nChina    1450\nIndia    1200\nUSA      1250\ndtype: int64\n\n\n\n# operation between series are also broadcasted\nprint(q1_series+q2_series)\n\nBrazil      NaN\nChina     950.0\nIndia     410.0\nJapan       NaN\nUSA       510.0\ndtype: float64\n\n\n\nprint(q1_series.add(q2_series, fill_value=0))\n\nBrazil    100.0\nChina     950.0\nIndia     410.0\nJapan      80.0\nUSA       510.0\ndtype: float64\n\n\n\n\n\n\nSebuah pd.DataFrame terdiri dari beberapa pd.Series yang berbagi nilai indeks.\n\nmy_data= np.random.randint(0,100,12).reshape(4,3)\nmy_data\n\narray([[25, 59, 18],\n       [75, 54, 65],\n       [29, 21,  7],\n       [32, 69, 16]])\n\n\nKita akan membuat pd.Dataframe melalui python list. Perhatikan bahwa kita dapat memberikan nama pada kolom dan baris\n\nmy_index= [\"jakarta\", \"bandung\", \"surabaya\", \"medan\"]\nmy_columns= [\"apple\", \"orange\", \"banana\"]\n\ndf= pd.DataFrame(data=my_data, index=my_index, columns=my_columns)\ndf\n\n\n\n\n\n  \n    \n      \n      apple\n      orange\n      banana\n    \n  \n  \n    \n      jakarta\n      25\n      59\n      18\n    \n    \n      bandung\n      75\n      54\n      65\n    \n    \n      surabaya\n      29\n      21\n      7\n    \n    \n      medan\n      32\n      69\n      16\n    \n  \n\n\n\n\n\ndf_2= pd.DataFrame(data=my_data)\ndf_2\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      25\n      59\n      18\n    \n    \n      1\n      75\n      54\n      65\n    \n    \n      2\n      29\n      21\n      7\n    \n    \n      3\n      32\n      69\n      16\n    \n  \n\n\n\n\n\ndf_3= pd.DataFrame(data=my_data, columns=my_columns)\ndf_3\n\n\n\n\n\n  \n    \n      \n      apple\n      orange\n      banana\n    \n  \n  \n    \n      0\n      25\n      59\n      18\n    \n    \n      1\n      75\n      54\n      65\n    \n    \n      2\n      29\n      21\n      7\n    \n    \n      3\n      32\n      69\n      16\n    \n  \n\n\n\n\n\n\nJika berkas .py atau .ipynb Anda berada di lokasi folder yang sama persis dengan berkas .csv yang ingin Anda baca, cukup berikan nama berkas sebagai string, misalnya:\ndf = pd.read_csv('[some_file.csv')\nBerikan s berkas jika Anda berada di direktori yang berbeda. Jalur berkas harus 100% benar agar ini berfungsi. Misalnya:\ndf = pd.read_csv(\"C:\\\\Users\\\\myself\\\\files\\\\some_file.csv\")\nsebelum itu, kalian dapat mendownload data tersebut melalui link berikut\nDownload\n\npwd\n\n'c:\\\\Users\\\\user\\\\Documents\\\\root\\\\personal\\\\github-personal\\\\sains-data-2023\\\\main-module'\n\n\n\ndf_tips= pd.read_csv('./data/tips.csv')\n\n\ndf_tips\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      239\n      29.03\n      5.92\n      Male\n      No\n      Sat\n      Dinner\n      3\n      9.68\n      Michael Avila\n      5296068606052842\n      Sat2657\n    \n    \n      240\n      27.18\n      2.00\n      Female\n      Yes\n      Sat\n      Dinner\n      2\n      13.59\n      Monica Sanders\n      3506806155565404\n      Sat1766\n    \n    \n      241\n      22.67\n      2.00\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n      11.34\n      Keith Wong\n      6011891618747196\n      Sat3880\n    \n    \n      242\n      17.82\n      1.75\n      Male\n      No\n      Sat\n      Dinner\n      2\n      8.91\n      Dennis Dixon\n      4375220550950\n      Sat17\n    \n    \n      243\n      18.78\n      3.00\n      Female\n      No\n      Thur\n      Dinner\n      2\n      9.39\n      Michelle Hardin\n      3511451626698139\n      Thur672\n    \n  \n\n244 rows × 11 columns\n\n\n\n\n\n\n\n# mengecek nama kolom\ndf_tips.columns\n\nIndex(['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size',\n       'price_per_person', 'Payer Name', 'CC Number', 'Payment ID'],\n      dtype='object')\n\n\n\n# mengecek \ndf_tips.index\n\nRangeIndex(start=0, stop=244, step=1)\n\n\n\ndf_tips.head(5)\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n    \n  \n\n\n\n\n\ndf_tips.tail(5)\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      239\n      29.03\n      5.92\n      Male\n      No\n      Sat\n      Dinner\n      3\n      9.68\n      Michael Avila\n      5296068606052842\n      Sat2657\n    \n    \n      240\n      27.18\n      2.00\n      Female\n      Yes\n      Sat\n      Dinner\n      2\n      13.59\n      Monica Sanders\n      3506806155565404\n      Sat1766\n    \n    \n      241\n      22.67\n      2.00\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n      11.34\n      Keith Wong\n      6011891618747196\n      Sat3880\n    \n    \n      242\n      17.82\n      1.75\n      Male\n      No\n      Sat\n      Dinner\n      2\n      8.91\n      Dennis Dixon\n      4375220550950\n      Sat17\n    \n    \n      243\n      18.78\n      3.00\n      Female\n      No\n      Thur\n      Dinner\n      2\n      9.39\n      Michelle Hardin\n      3511451626698139\n      Thur672\n    \n  \n\n\n\n\n\ndf_tips.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      total_bill\n      244.0\n      1.978594e+01\n      8.902412e+00\n      3.070000e+00\n      1.334750e+01\n      1.779500e+01\n      2.412750e+01\n      5.081000e+01\n    \n    \n      tip\n      244.0\n      2.998279e+00\n      1.383638e+00\n      1.000000e+00\n      2.000000e+00\n      2.900000e+00\n      3.562500e+00\n      1.000000e+01\n    \n    \n      size\n      244.0\n      2.569672e+00\n      9.510998e-01\n      1.000000e+00\n      2.000000e+00\n      2.000000e+00\n      3.000000e+00\n      6.000000e+00\n    \n    \n      price_per_person\n      244.0\n      7.888197e+00\n      2.914234e+00\n      2.880000e+00\n      5.800000e+00\n      7.255000e+00\n      9.390000e+00\n      2.027000e+01\n    \n    \n      CC Number\n      244.0\n      2.563496e+15\n      2.369340e+15\n      6.040679e+10\n      3.040731e+13\n      3.525318e+15\n      4.553675e+15\n      6.596454e+15\n    \n  \n\n\n\n\n\n\n\n\n\n\n\ndf_tips.head(5)\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n    \n  \n\n\n\n\n\nprint(df_tips[\"size\"] ==3)\nconditional_size = df_tips[\"size\"] ==3\n\n0      False\n1       True\n2       True\n3      False\n4      False\n       ...  \n239     True\n240    False\n241    False\n242    False\n243    False\nName: size, Length: 244, dtype: bool\n\n\n\ndf_tips[conditional_size].head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      16\n      10.33\n      1.67\n      Female\n      No\n      Sun\n      Dinner\n      3\n      3.44\n      Elizabeth Foster\n      4240025044626033\n      Sun9715\n    \n    \n      17\n      16.29\n      3.71\n      Male\n      No\n      Sun\n      Dinner\n      3\n      5.43\n      John Pittman\n      6521340257218708\n      Sun2998\n    \n    \n      18\n      16.97\n      3.50\n      Female\n      No\n      Sun\n      Dinner\n      3\n      5.66\n      Laura Martinez\n      30422275171379\n      Sun2789\n    \n  \n\n\n\n\n\nconditional= (df_tips[\"size\"]==3) & (df_tips[\"total_bill\"]>20)\nprint(conditional)\n\n0      False\n1      False\n2       True\n3      False\n4      False\n       ...  \n239     True\n240    False\n241    False\n242    False\n243    False\nLength: 244, dtype: bool\n\n\n\ndf_tips[conditional].head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      19\n      20.65\n      3.35\n      Male\n      No\n      Sat\n      Dinner\n      3\n      6.88\n      Timothy Oneal\n      6568069240986485\n      Sat9213\n    \n    \n      35\n      24.06\n      3.60\n      Male\n      No\n      Sat\n      Dinner\n      3\n      8.02\n      Joseph Mullins\n      5519770449260299\n      Sat632\n    \n    \n      39\n      31.27\n      5.00\n      Male\n      No\n      Sat\n      Dinner\n      3\n      10.42\n      Mr. Brandon Berry\n      6011525851069856\n      Sat6373\n    \n    \n      48\n      28.55\n      2.05\n      Male\n      No\n      Sun\n      Dinner\n      3\n      9.52\n      Austin Fisher\n      6011481668986587\n      Sun4142\n    \n  \n\n\n\n\n\ndf_tips[(df_tips[\"size\"]==3) & (df_tips[\"total_bill\"]>20)].head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      19\n      20.65\n      3.35\n      Male\n      No\n      Sat\n      Dinner\n      3\n      6.88\n      Timothy Oneal\n      6568069240986485\n      Sat9213\n    \n    \n      35\n      24.06\n      3.60\n      Male\n      No\n      Sat\n      Dinner\n      3\n      8.02\n      Joseph Mullins\n      5519770449260299\n      Sat632\n    \n    \n      39\n      31.27\n      5.00\n      Male\n      No\n      Sat\n      Dinner\n      3\n      10.42\n      Mr. Brandon Berry\n      6011525851069856\n      Sat6373\n    \n    \n      48\n      28.55\n      2.05\n      Male\n      No\n      Sun\n      Dinner\n      3\n      9.52\n      Austin Fisher\n      6011481668986587\n      Sun4142\n    \n  \n\n\n\n\n\nweekend= [\"Sun\", \"Sat\"]\nconditional_in= df_tips[\"day\"].isin(weekend)\ndf_tips[conditional_in].head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n    \n  \n\n\n\n\n\ndf_tips.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n    \n  \n\n\n\n\n\n\n\n\ndf_tips[\"day\"].unique()\n\narray(['Sun', 'Sat', 'Thur', 'Fri'], dtype=object)\n\n\n\ndf_tips.drop_duplicates([\"day\",\"time\"])[[\"day\",\"time\"]]\n\n\n\n\n\n  \n    \n      \n      day\n      time\n    \n  \n  \n    \n      0\n      Sun\n      Dinner\n    \n    \n      19\n      Sat\n      Dinner\n    \n    \n      77\n      Thur\n      Lunch\n    \n    \n      90\n      Fri\n      Dinner\n    \n    \n      220\n      Fri\n      Lunch\n    \n    \n      243\n      Thur\n      Dinner\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nprint(df_tips[\"day\"])\nprint(\"=======\")\nprint(df_tips.day)\n\n0       Sun\n1       Sun\n2       Sun\n3       Sun\n4       Sun\n       ... \n239     Sat\n240     Sat\n241     Sat\n242     Sat\n243    Thur\nName: day, Length: 244, dtype: object\n=======\n0       Sun\n1       Sun\n2       Sun\n3       Sun\n4       Sun\n       ... \n239     Sat\n240     Sat\n241     Sat\n242     Sat\n243    Thur\nName: day, Length: 244, dtype: object\n\n\n\ndf_tips[[\"day\",\"time\"]]\n\n\n\n\n\n  \n    \n      \n      day\n      time\n    \n  \n  \n    \n      0\n      Sun\n      Dinner\n    \n    \n      1\n      Sun\n      Dinner\n    \n    \n      2\n      Sun\n      Dinner\n    \n    \n      3\n      Sun\n      Dinner\n    \n    \n      4\n      Sun\n      Dinner\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      239\n      Sat\n      Dinner\n    \n    \n      240\n      Sat\n      Dinner\n    \n    \n      241\n      Sat\n      Dinner\n    \n    \n      242\n      Sat\n      Dinner\n    \n    \n      243\n      Thur\n      Dinner\n    \n  \n\n244 rows × 2 columns\n\n\n\n\n\n\n\ndf_tips[\"tips_percentage\"]= df_tips[\"tip\"]/df_tips[\"total_bill\"]*100\n\ndf_tips.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n      tips_percentage\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n      5.944673\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n      16.054159\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n      16.658734\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n      13.978041\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n      14.680765\n    \n  \n\n\n\n\n\n\n\n\ndf_tips.rename(columns={\"tips_percentage\":\"tips_percentage_%\"}, inplace=True)\ndf_tips.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n      Payment ID\n      tips_percentage_%\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n      Sun2959\n      5.944673\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n      Sun4608\n      16.054159\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n      Sun4458\n      16.658734\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n      Sun5260\n      13.978041\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n      Sun2251\n      14.680765\n    \n  \n\n\n\n\n\n\n\n\n#relocate tips_percentage_% column to the rightmost\ncols= list(df_tips.columns)\ncols= [cols[-1]]+ cols[:-2]\n\ndf_tips= df_tips[cols]\n\n\ndf_tips\n\n\n\n\n\n  \n    \n      \n      tips_percentage_%\n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      price_per_person\n      Payer Name\n      CC Number\n    \n  \n  \n    \n      0\n      5.944673\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n      8.49\n      Christy Cunningham\n      3560325168603410\n    \n    \n      1\n      16.054159\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n      3.45\n      Douglas Tucker\n      4478071379779230\n    \n    \n      2\n      16.658734\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n      7.00\n      Travis Walters\n      6011812112971322\n    \n    \n      3\n      13.978041\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n      11.84\n      Nathaniel Harris\n      4676137647685994\n    \n    \n      4\n      14.680765\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n      6.15\n      Tonya Carter\n      4832732618637221\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      239\n      20.392697\n      29.03\n      5.92\n      Male\n      No\n      Sat\n      Dinner\n      3\n      9.68\n      Michael Avila\n      5296068606052842\n    \n    \n      240\n      7.358352\n      27.18\n      2.00\n      Female\n      Yes\n      Sat\n      Dinner\n      2\n      13.59\n      Monica Sanders\n      3506806155565404\n    \n    \n      241\n      8.822232\n      22.67\n      2.00\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n      11.34\n      Keith Wong\n      6011891618747196\n    \n    \n      242\n      9.820426\n      17.82\n      1.75\n      Male\n      No\n      Sat\n      Dinner\n      2\n      8.91\n      Dennis Dixon\n      4375220550950\n    \n    \n      243\n      15.974441\n      18.78\n      3.00\n      Female\n      No\n      Thur\n      Dinner\n      2\n      9.39\n      Michelle Hardin\n      3511451626698139\n    \n  \n\n244 rows × 11 columns"
  },
  {
    "objectID": "main-module/week-02.html",
    "href": "main-module/week-02.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Pada modul ini kita akan mempelajari beberapa cara untuk membuat visualisasi data menggunakan package Matplotlib dan Seaborn. Seaborn merupakan salah satu package visualisasi data yang sangat sering digunakan karena fleksibilitas dan banyaknya jenis plot yang disediakan.\n\n\n\n\nSebelum memulai, mari kita import terlebih dahulu module - module yang diperlukan.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nPada module kali ini, akan digunakan tiga data csv yang berbeda untuk mempermudah kebutuhan visualisasi.\nKetiga data tersebut dapat kalian unduh pada tautan berikut: https://bit.ly/DataWeek2\n\nspotify_df = pd.read_csv('data/week 2/spotify.csv', index_col='Date', parse_dates=['Date'])\nflight_df = pd.read_csv('data/week 2/flight_delays.csv')\ninsurance_df = pd.read_csv('data/week 2/insurance.csv')\n\n\n\n\n\nSeperti yang sudah dipelajari pada Algoritma dan Pemrograman, visualisasi data dapat dilakukan dengan module matplotlib, antara lain untuk membuat line plot dan scatter plot.\nPertama, kita akan menggunakan data Spotify, yaitu data total daily streams 5 lagu hits pada masanya.\n\nspotify_df\n\n\n\n\n\n  \n    \n      \n      Shape of You\n      Despacito\n      Something Just Like This\n      HUMBLE.\n      Unforgettable\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017-01-06\n      12287078\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2017-01-07\n      13190270\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2017-01-08\n      13099919\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2017-01-09\n      14506351\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2017-01-10\n      14275628\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-01-05\n      4492978\n      3450315.0\n      2408365.0\n      2685857.0\n      2869783.0\n    \n    \n      2018-01-06\n      4416476\n      3394284.0\n      2188035.0\n      2559044.0\n      2743748.0\n    \n    \n      2018-01-07\n      4009104\n      3020789.0\n      1908129.0\n      2350985.0\n      2441045.0\n    \n    \n      2018-01-08\n      4135505\n      2755266.0\n      2023251.0\n      2523265.0\n      2622693.0\n    \n    \n      2018-01-09\n      4168506\n      2791601.0\n      2058016.0\n      2727678.0\n      2627334.0\n    \n  \n\n366 rows × 5 columns\n\n\n\nBerikut adalah cara untuk membuat line plot pada satu fitur di dataframe menggunakan matplotlib\n\n\"\"\"\nMembuat line plot untuk lagu Shape of You menggunakan matplotlib\n\"\"\"\n\n# Mengatur besar figur plot\nplt.subplots(figsize=(8,6))\n\n# Membuat line plot\nplt.plot(spotify_df['Shape of You'], 'b')\n# Membuat label sumbu-x dan sumbu-y\nplt.xlabel('Date')\nplt.ylabel('Shape of You Total Daily Streams')\n# Menampilkan plot\nplt.show()\n\n\n\n\nApabila kita ingin menampilkan fitur-fitur lain dalam figur yang sama, kita dapat memanfaatkan loop\n\n\"\"\"\nMembuat line plot untuk semua lagu dalam spotify_df menggunakan loop\n\"\"\"\n\nplt.subplots(figsize=(8,6))\n\n# Loop setiap nama kolom pada dataframe, lalu plot\nfor column in spotify_df.columns:\n    plt.plot(spotify_df[column])\n\nplt.legend(spotify_df.columns)\nplt.show()\n\n\n\n\nNamun, terdapat cara yang lebih mudah selain menggunakan looping. pandas dataframe memiliki method yang dapat secara langsung memvisualisasikan keseluruhan fiturnya, yaitu .plot().\nPada .plot() kita memiliki beberapa parameter yang dapat diatur, antara lain kind dan figsize. kind berfungsi untuk mengatur jenis plot yang ingin kita buat, sedangkan figsize berfungsi untuk mengatur besar figur yang dihasilkan.\nParameter lainnya dapat dilihat pada: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n\n\"\"\"\nMembuat line plot untuk semua lagu dalam spotify_df menggunakan pandas .plot()\n\"\"\"\n\nspotify_df.plot(kind='line', figsize=(8,6))\nplt.xlabel('Date')\nplt.ylabel('Total Daily Streams')\nplt.show()\n\n\n\n\nSelain line plot, terdapat banyak macam kind yang bisa digunakan. Pada code cell dibawah terlihat bahwa pandas .plot() dapat menghasilkan histogram (perlu diperhatikan bahwa jenis plot perlu menyesuaikan tipe data yang dimiliki, terlihat bahwa menggunakan data spotify, histogram tidak menghasilkan insight yang cukup berguna).\n\nspotify_df.plot(kind='hist', figsize=(8,6), alpha=.7)\n\nplt.show()\n\n\n\n\nPada praktikum Algoritma dan Pemrograman kita juga telah mempelajari cara untuk membuat scatter plot. Berikut code untuk membuat scatter plot menggunakan matplotlib, untuk melihat korelasi antara daily streams lagu Shape of You dengan Something Just Like This.\n\n\"\"\"\nMembuat scatter plot untuk melihat korelasi antara lagu\nShape of You dengan Something Just Like This menggunakan\nmatplotlib\n\"\"\"\n\nplt.subplots(figsize=(8,6))\n\nplt.scatter(x=spotify_df['Shape of You'], \n            y=spotify_df['Something Just Like This'],\n            alpha=.5)\nplt.xlabel('\"Shape of You\" Total Daily Streams')\nplt.ylabel('\"Something Just Like This\" Total Daily Streams')\nplt.show()\n\n\n\n\n\n\n\nWalaupun matplotlib cukup fleksibel dalam menghasilkan plot, tetapi tipe plot yang disediakan cenderung terbatas. Oleh karena itu, kita dapat menggunakan Seaborn karena tipe plot yang disediakan sangat banyak sesuai kebutuhan kita, antara lain line, bar, heatmap, scatter, box, swarm, histogram, density, dan masih banyak lagi.\n\n\nLine plot biasa digunakan untuk melihat trend data dalam jangka waktu tertentu.\nUntuk membuat line plot pada seaborn, kita dapat menggunakan sns.lineplot(). Jika data yang ingin kita visualisasikan adalah dataframe, kita dapat memasukkan variabel dataframe tersebut pada parameter data, seperti code di bawah ini.\n\n\"\"\"\nMembuat line plot dengan module seaborn\n\"\"\"\n\nplt.subplots(figsize=(8,6))\nsns.lineplot(data=spotify_df)\nplt.show()\n\n\n\n\nFleksibilitas Seaborn membuat kita dapat memilih color palette yang sesuai dengan keinginan kita. Kita dapat memilih palette yang sudah disediakan oleh seaborn (antara lain: bright, deep, pastel, dan masih banyak lagi) atau kita dapat mengatur sendiri palette yang ingin kita gunakan.\nUntuk memilih palette yang akan digunakan untuk plot selanjutnya pada seaborn, kita dapat menggunakan sns.set_palette().\nJenis palette yang disediakan seaborn serta cara membuat color palette secara mandiri dapat dilihat pada: https://seaborn.pydata.org/tutorial/color_palettes.html#tools-for-choosing-color-palettes\n\n# Mengganti color palette menjadi \"bright\"\nsns.set_palette('bright')\n\n\n\"\"\"\nMembuat line plot setelah color palette diubah menjadi \"bright\"\n\"\"\"\n\n# Mengatur besar figur yang ingin ditampilkan\nplt.figure(figsize=(14,6))\n\n# Membuat line plot\nsns.lineplot(data=spotify_df)\n# Membuat judul figur\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\n# Menampilkan plot\nplt.show()\n\n\n\n\nApabila tidak semua fitur pada data ingin kita visualisasikan, kita dapat menggunakan sns.lineplot() beberapa kali, sesuai dengan banyaknya fitur yang ingin kita tampilkan, seperti pada code di bawah.\n\nplt.figure(figsize=(14,6))\n\n# Membuat line plot hanya dengan lagu Shape of You\nsns.lineplot(data=spotify_df['Shape of You'], label=\"Shape of You\")\n# Menambahkan line plot pada figur dengan lagu Despacito\nsns.lineplot(data=spotify_df['Despacito'], label=\"Despacito\")\n\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\nplt.xlabel(\"Date\")\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\nBar plot biasa digunakan untuk membandingkan kuantitas/nilai pada data bertipe kategori.\nSelanjutnya, kita akan menggunakan data flight_delays.csv, yaitu data rata-rata keterlambatan beberapa maskapai pesawat pada setiap bulannya.\n\nflight_df\n\n\n\n\n\n  \n    \n      \n      Month\n      AA\n      AS\n      B6\n      DL\n      EV\n      F9\n      HA\n      MQ\n      NK\n      OO\n      UA\n      US\n      VX\n      WN\n    \n  \n  \n    \n      0\n      1\n      6.955843\n      -0.320888\n      7.347281\n      -2.043847\n      8.537497\n      18.357238\n      3.512640\n      18.164974\n      11.398054\n      10.889894\n      6.352729\n      3.107457\n      1.420702\n      3.389466\n    \n    \n      1\n      2\n      7.530204\n      -0.782923\n      18.657673\n      5.614745\n      10.417236\n      27.424179\n      6.029967\n      21.301627\n      16.474466\n      9.588895\n      7.260662\n      7.114455\n      7.784410\n      3.501363\n    \n    \n      2\n      3\n      6.693587\n      -0.544731\n      10.741317\n      2.077965\n      6.730101\n      20.074855\n      3.468383\n      11.018418\n      10.039118\n      3.181693\n      4.892212\n      3.330787\n      5.348207\n      3.263341\n    \n    \n      3\n      4\n      4.931778\n      -3.009003\n      2.780105\n      0.083343\n      4.821253\n      12.640440\n      0.011022\n      5.131228\n      8.766224\n      3.223796\n      4.376092\n      2.660290\n      0.995507\n      2.996399\n    \n    \n      4\n      5\n      5.173878\n      -1.716398\n      -0.709019\n      0.149333\n      7.724290\n      13.007554\n      0.826426\n      5.466790\n      22.397347\n      4.141162\n      6.827695\n      0.681605\n      7.102021\n      5.680777\n    \n    \n      5\n      6\n      8.191017\n      -0.220621\n      5.047155\n      4.419594\n      13.952793\n      19.712951\n      0.882786\n      9.639323\n      35.561501\n      8.338477\n      16.932663\n      5.766296\n      5.779415\n      10.743462\n    \n    \n      6\n      7\n      3.870440\n      0.377408\n      5.841454\n      1.204862\n      6.926421\n      14.464543\n      2.001586\n      3.980289\n      14.352382\n      6.790333\n      10.262551\n      NaN\n      7.135773\n      10.504942\n    \n    \n      7\n      8\n      3.193907\n      2.503899\n      9.280950\n      0.653114\n      5.154422\n      9.175737\n      7.448029\n      1.896565\n      20.519018\n      5.606689\n      5.014041\n      NaN\n      5.106221\n      5.532108\n    \n    \n      8\n      9\n      -1.432732\n      -1.813800\n      3.539154\n      -3.703377\n      0.851062\n      0.978460\n      3.696915\n      -2.167268\n      8.000101\n      1.530896\n      -1.794265\n      NaN\n      0.070998\n      -1.336260\n    \n    \n      9\n      10\n      -0.580930\n      -2.993617\n      3.676787\n      -5.011516\n      2.303760\n      0.082127\n      0.467074\n      -3.735054\n      6.810736\n      1.750897\n      -2.456542\n      NaN\n      2.254278\n      -0.688851\n    \n    \n      10\n      11\n      0.772630\n      -1.916516\n      1.418299\n      -3.175414\n      4.415930\n      11.164527\n      -2.719894\n      0.220061\n      7.543881\n      4.925548\n      0.281064\n      NaN\n      0.116370\n      0.995684\n    \n    \n      11\n      12\n      4.149684\n      -1.846681\n      13.839290\n      2.504595\n      6.685176\n      9.346221\n      -1.706475\n      0.662486\n      12.733123\n      10.947612\n      7.012079\n      NaN\n      13.498720\n      6.720893\n    \n  \n\n\n\n\nUntuk membuat bar plot pada seaborn dengan dataframe, kita dapat menggunakan sns.barplot() dengan tiga parameter yang wajib kita set, yaitu:\n- data: dataframe yang ingin kita visualisasikan\n\n- x: nama fitur pada dataframe yang ingin kita jadikan sumbu-x\n\n- y: nama fitur pada dataframe yang ingin kita jadikan sumbu-y\nPada kode di bawah, juga digunakan satu parameter opsional, yaitu palette yang merupakan cara lain untuk mengatur color palette yang ingin kita gunakan\n\n\"\"\"\nMembuat bar plot keterlambatan maskapai EV setiap \nbulannya menggunakan seaborn\n\"\"\"\n\nplt.figure(figsize=(14,6))\n\nsns.barplot(data=flight_df, x='Month', y='EV',\n            palette=sns.color_palette('deep'))\nplt.ylabel('EV Flight Delays (minute)')\nplt.title('Average EV Flight Delays per Month')\nplt.show()\n\n\n\n\nBerdasarkan hasil plot di atas, terlihat bahwa maskapai EV memiliki rata-rata keterlambatan terlama pada bulan Juni, serta tercepat pada bulan September.\nSelanjutnya, mari kita coba lihat urutan rata-rata keterlambatan semua maskapai dalam satu tahun (maskapai mana yang memiliki rata-rata keterlambatan terlama, serta maskapai mana yang tercepat).\nHal pertama yang perlu kita lakukan adalah, jadikan fitur Month sebagai index dataframe.\n\n# Set fitur \"Month\" menjadi index dataframe\nflight_df = flight_df.set_index('Month')\nflight_df.head(2)\n\n\n\n\n\n  \n    \n      \n      AA\n      AS\n      B6\n      DL\n      EV\n      F9\n      HA\n      MQ\n      NK\n      OO\n      UA\n      US\n      VX\n      WN\n    \n    \n      Month\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      6.955843\n      -0.320888\n      7.347281\n      -2.043847\n      8.537497\n      18.357238\n      3.512640\n      18.164974\n      11.398054\n      10.889894\n      6.352729\n      3.107457\n      1.420702\n      3.389466\n    \n    \n      2\n      7.530204\n      -0.782923\n      18.657673\n      5.614745\n      10.417236\n      27.424179\n      6.029967\n      21.301627\n      16.474466\n      9.588895\n      7.260662\n      7.114455\n      7.784410\n      3.501363\n    \n  \n\n\n\n\nSelanjutnya, kita perlu hitung rata-rata keterlambatan tiap maskapai dalam satu tahun, yaitu hitung rata-rata tiap kolom pada dataframe menggunakan .mean() (Tambahan: apabila kita ingin menghitung rata-rata tiap barisnya, kita dapat menggunakan parameter axis=1 pada .mean()). .mean() akan menghasilkan pandas Series.\nLalu, agar mempermudah kita dalam melihat visualisasi bar plotnya, kita dapat menggunakan .sort_values().\n\n# Simpan rata-rata keterlambatan semua maskapai dalam satu tahun pada variabel flight_mean_inyear\nflight_mean_inyear = flight_df.mean()\n# Urutkan flight_mean_inyear secara ascending\nflight_mean_inyear = flight_mean_inyear.sort_values()\n\nflight_mean_inyear\n\nAS    -1.023656\nDL     0.231116\nHA     1.993205\nUS     3.776815\nAA     4.120776\nWN     4.275277\nVX     4.717718\nUA     5.413415\nOO     5.909658\nMQ     5.964953\nEV     6.543328\nB6     6.788370\nF9    13.035736\nNK    14.549663\ndtype: float64\n\n\nTerakhir, visualisasikan bar plot menggunakan cara seperti sebelumnya.\nKita dapat lihat pada code dibawah bahwa tidak digunakan parameter data, karena flight_mean_inyear merupakan pandas Series (bukan dataframe) sehingga lebih mudah jika kita langsung menggunakan parameter x dan y saja.\n\nplt.subplots(figsize=(14,6))\nsns.barplot(x=flight_mean_inyear.index, \n            y=flight_mean_inyear.values,\n            palette=sns.color_palette('deep'))\nplt.title('Average Delay per Flight in a Year')\nplt.show()\n\n\n\n\nBerdasarkan plot diatas, NK merupakan maskapai dengan rata-rata keterlambatan terlama dalam satu tahun, sedangkan AS adalah yang tercepat (AS bernilai negatif yang berarti rata-rata kedatangan pesawat lebih cepat dari yang dijadwalkan dalam satu tahun.\n\n\n\nHeatmap biasa digunakan untuk mempermudah melihat pola pada data berdasarkan warna yang dihasilkan.\nPada seaborn, kita dapat menggunakan heatmap dengan sns.heatmap() seperti pada kode dibawah. Parameter annot berfungsi untuk menampilkan nilai data (jika True) atau tidak (jika False).\nBar sebelah kanan heatmap menunjukkan bahwa, semakin lama keterlambatan pesawat, maka warna yang dihasilkan semakin terang. Sebaliknya, semakin gelap warna yang dihasilkan berarti semakin cepat pesawat datang tersebut.\n\n\"\"\"\nMembuat heatmap menggunakan Seaborn\n\"\"\"\nplt.figure(figsize=(14,10))\n\nsns.heatmap(data=flight_df, annot=True)\nplt.title(\"Average Arrival Delay for Each Airline, by Month\")\nplt.xlabel(\"Airline\")\nplt.show()\n\n\n\n\nBerdasarkan heatmap di atas, kita dapat melihat dengan mudah pada bulan apa suatu maskapai sangat terlambat (contoh: maskapai NK pada bulan Juni).\nHeatmap sangat sering digunakan untuk melihat korelasi antarfitur pada dataset agar kita dapat mengerti lebih jauh tentang fitur-fitur pada data, atau juga dapat dimanfaatkan untuk melakukan feature selection sebelum membuat sebuat model Machine Learning.\nUntuk melakukan hal tersebut, kita perlu menghitung dahulu korelasi antar fitur menggunakan pandas .corr(), yaitu fungsi yang akan menghitung korelasi antar dua fitur menggunakan korelasi Pearson.\nNotes: Metode korelasi dapat diubah dengan menggunakan parameter method pada .corr(), contoh: .corr(method='spearman'). Metode lainnya dapat dilihat pada: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n\n# Hitung korelasi antar dua fitur pada flight_df\nflight_corr = flight_df.corr()\n\nflight_corr\n\n\n\n\n\n  \n    \n      \n      AA\n      AS\n      B6\n      DL\n      EV\n      F9\n      HA\n      MQ\n      NK\n      OO\n      UA\n      US\n      VX\n      WN\n    \n  \n  \n    \n      AA\n      1.000000\n      0.334980\n      0.429854\n      0.805229\n      0.896523\n      0.903986\n      0.220065\n      0.842701\n      0.573716\n      0.620477\n      0.809874\n      0.823713\n      0.425237\n      0.615664\n    \n    \n      AS\n      0.334980\n      1.000000\n      0.340359\n      0.394359\n      0.356608\n      0.336791\n      0.684979\n      0.283977\n      0.480863\n      0.350657\n      0.457414\n      0.489025\n      0.229571\n      0.519228\n    \n    \n      B6\n      0.429854\n      0.340359\n      1.000000\n      0.643313\n      0.342627\n      0.510718\n      0.467905\n      0.529724\n      0.032038\n      0.591115\n      0.233021\n      0.788345\n      0.579750\n      0.151750\n    \n    \n      DL\n      0.805229\n      0.394359\n      0.643313\n      1.000000\n      0.796951\n      0.783265\n      0.262251\n      0.598765\n      0.625277\n      0.569073\n      0.797339\n      0.821757\n      0.700605\n      0.691805\n    \n    \n      EV\n      0.896523\n      0.356608\n      0.342627\n      0.796951\n      1.000000\n      0.828515\n      0.099369\n      0.721468\n      0.784026\n      0.692697\n      0.911499\n      0.669736\n      0.462638\n      0.730115\n    \n    \n      F9\n      0.903986\n      0.336791\n      0.510718\n      0.783265\n      0.828515\n      1.000000\n      0.273878\n      0.912984\n      0.414064\n      0.582509\n      0.671986\n      0.878874\n      0.308397\n      0.465765\n    \n    \n      HA\n      0.220065\n      0.684979\n      0.467905\n      0.262251\n      0.099369\n      0.273878\n      1.000000\n      0.436015\n      0.176485\n      0.056941\n      0.066821\n      0.586160\n      -0.008439\n      -0.007296\n    \n    \n      MQ\n      0.842701\n      0.283977\n      0.529724\n      0.598765\n      0.721468\n      0.912984\n      0.436015\n      1.000000\n      0.281890\n      0.586963\n      0.503575\n      0.660181\n      0.150111\n      0.239744\n    \n    \n      NK\n      0.573716\n      0.480863\n      0.032038\n      0.625277\n      0.784026\n      0.414064\n      0.176485\n      0.281890\n      1.000000\n      0.365273\n      0.827455\n      0.293515\n      0.395419\n      0.742869\n    \n    \n      OO\n      0.620477\n      0.350657\n      0.591115\n      0.569073\n      0.692697\n      0.582509\n      0.056941\n      0.586963\n      0.365273\n      1.000000\n      0.626051\n      0.590313\n      0.561515\n      0.548304\n    \n    \n      UA\n      0.809874\n      0.457414\n      0.233021\n      0.797339\n      0.911499\n      0.671986\n      0.066821\n      0.503575\n      0.827455\n      0.626051\n      1.000000\n      0.477816\n      0.536968\n      0.926800\n    \n    \n      US\n      0.823713\n      0.489025\n      0.788345\n      0.821757\n      0.669736\n      0.878874\n      0.586160\n      0.660181\n      0.293515\n      0.590313\n      0.477816\n      1.000000\n      0.333396\n      0.242344\n    \n    \n      VX\n      0.425237\n      0.229571\n      0.579750\n      0.700605\n      0.462638\n      0.308397\n      -0.008439\n      0.150111\n      0.395419\n      0.561515\n      0.536968\n      0.333396\n      1.000000\n      0.630278\n    \n    \n      WN\n      0.615664\n      0.519228\n      0.151750\n      0.691805\n      0.730115\n      0.465765\n      -0.007296\n      0.239744\n      0.742869\n      0.548304\n      0.926800\n      0.242344\n      0.630278\n      1.000000\n    \n  \n\n\n\n\nPandas .corr() menghasilkan dataframe dengan nama baris dan kolom yang sama, serta berisi nilai korelasi antara baris dan kolom yang ditinjau (contoh: korelasi antara maskapai AA dan AS adalah 0,334980). Serta, dataframe yang dihasilkan adalah sebuat matriks simetris.\nTentu dengan hanya melihat dataframe di atas, tidak terlihat begitu jelas mana fitur yang memiliki korelasi tinggi dan mana yang yang memiliki korelasi rendah. Oleh karena itu, kita dapat memanfaatkan heatmap.\nPada code di bawah, untuk mempermudah pembacaan heatmap, kita menggunakan parameter vmin, vmax, dan center pada sns.heatmap(). vmin berfungsi untuk mengatur nilai terendah, vmax berfungsi untuk mengatur nilai tertinggi, dan center berfungsi untuk mengatur nilai tengah pada heatmap. Korelasi Pearson menghasilkan nilai antara -1 hingga 1, sehingga kita dapat set ketiga parameter tersebut seperti pada code di bawah.\n\nplt.figure(figsize=(14,10))\n\nsns.heatmap(data=flight_corr, vmin=-1, vmax=1, center=0, annot=True)\nplt.title(\"Pearson Correlation of Each Airline Flight Delays\")\nplt.xlabel(\"Airline\")\nplt.show()\n\n\n\n\nDengan menggunakan heatmap, sekarang terlihat bahwa mana maskapai yang keterlambatannya berkorelasi tinggi dan mana yang rendah. Misal, AA dan EV menghasilkan korelasi yang cukup tinggi positif, yaitu 0.9, yang artinya jika keterlambatan maskapai AA tinggi, begitu juga maskapai EV, dan sebaliknya jika keterlambatan maskapai AA rendah, begitu juga maskapai EV.\nUntuk meyakinkan kita dengan hal tersebut, kita dapat lihat pada materi selanjutnya, yaitu Scatter Plot.\n\n\n\nScatter plot biasa digunakan untuk melihat korelasi antara dua fitur bertipe numerik.\nUntuk menggunakan scatter plot pada seaborn, kita dapat menggunakan sns.scatterplot(), dengan parameter yang sama seperti kita membuat bar plot.\n\n\"\"\"\nMembuat scatter plot untuk melihat \nketerkaitan pada keterlambatan pesawat\nmaskapai EV dan AA\n\"\"\"\n\nsns.scatterplot(data=flight_df, x='EV', y='AA')\nplt.show()\n\n\n\n\nMelalui scatter plot di atas, kita dapat semakin yakin bahwa kesimpulan yang kita ambil dengan melihat heatmap sebelumnya benar.\n\n\"\"\"\nTambahan scatter plot pada maskapai lain yang\nmemiliki korelasi tinggi\n\"\"\"\n\nsns.scatterplot(data=flight_df, x='EV', y='UA')\nplt.show()\n\n\n\n\n\n\"\"\"\nScatter plot pada maskapai yang memiliki\nkorelasi rendah (mendekati 0)\n\"\"\"\n\nsns.scatterplot(data=flight_df, x='UA', y='HA')\nplt.show()\n\n\n\n\nPada heatmap, terlihat bahwa maskapai UA dan HA memiliki korelasi yang rendah, yaitu 0.067. Sehingga, jika kita buat scatter plotnya, menghasilkan plot seperti di atas.\nUntuk memahami scatter plot lebih baik, kita akan menggunakan dataset lainnya, yaitu insurance.csv yang merupakan data berisi biaya asuransi (charges) beberapa orang.\n\ninsurance_df.head()\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      bmi\n      children\n      smoker\n      region\n      charges\n    \n  \n  \n    \n      0\n      19\n      female\n      27.900\n      0\n      yes\n      southwest\n      16884.92400\n    \n    \n      1\n      18\n      male\n      33.770\n      1\n      no\n      southeast\n      1725.55230\n    \n    \n      2\n      28\n      male\n      33.000\n      3\n      no\n      southeast\n      4449.46200\n    \n    \n      3\n      33\n      male\n      22.705\n      0\n      no\n      northwest\n      21984.47061\n    \n    \n      4\n      32\n      male\n      28.880\n      0\n      no\n      northwest\n      3866.85520\n    \n  \n\n\n\n\nMisal, kita ingin melihat keterkaitan indeks massa tubuh (bmi) seseorang dengan biaya asuransi (charges) orang tersebut. Sama seperti sebelumnya, kita dapat melakukannya seperti pada code di bawah.\n\n# Mengubah palette menjadi default\nsns.set_palette('tab10')\n# Membuat scatter plot antara fitur bmi dan charges\nsns.scatterplot(data=insurance_df, x='bmi', y='charges')\n\nplt.show()\n\n\n\n\nScatter plot di atas menunjukkan bahwa korelasi antara bmi dan charges adalah cenderung positif, tetapi tidak terlalu tinggi. Yang artinya, orang dengan BMI tinggi, cenderung akan membayar biaya asuransi lebih tinggi.\nAgar kita semakin yakin dengan kesimpulan tersebut, kita dapat menambahakn garis regresi pada scatter plot tersebut dengan menggunakan sns.regplot().\n\nsns.regplot(data=insurance_df, x='bmi', y='charges')\nplt.show()\n\n\n\n\nBerdasarkan scatter plot dan garis regresi dihasilkan, terlihat bahwa kesimpulan yang kita ambil benar. Agar semakin yakin lagi, kita juga dapat menghitung langsung korelasi Pearsonnya menggunakan cara sebelumnya, yaitu pandas .corr().\n\ninsurance_df[['bmi', 'charges']].corr()\n\n\n\n\n\n  \n    \n      \n      bmi\n      charges\n    \n  \n  \n    \n      bmi\n      1.000000\n      0.198341\n    \n    \n      charges\n      0.198341\n      1.000000\n    \n  \n\n\n\n\nDengan menggunakan seaborn, kita juga dapat memvisualisasikan scatter plot berdasarkan dengan pewarnaan yang berbeda berdasarkan fitur lainnya yang bertipe kategorik.\nMisal, kita ingin membuat scatter plot antara fitur bmi dan charges dengan pewarnaannya berdasarkan nilai dari fitur smoker, yaitu yes atau no. Kita dapat set parameter hue='smoker' pada sns.scatterplot() seperti pada code di bawah.\n\nsns.scatterplot(data=insurance_df, x='bmi', y='charges', hue='smoker')\nplt.show()\n\n\n\n\nSehingga dihasilkan pewarnaan yang berbeda untuk seseorang yang merupakan perokok (biru) dan yang tidak (orange). Berdasarkan scatter plot di atas, terlihat bahwa korelasi antara bmi dan charges untuk perokok cendering tinggi positif (semakin besar bmi, semakin besar juga charges). Sedangkan, untuk bukan perokok, korelasinya cenderung rendah (semakin besar bmi, tidak terlalu berpengaruh terhadap charges).\nSeperti cara sebelumnya, kita dapat menambahkan garis regresi. Namun, karena kita disini menggunakan hue, terdapat dua cara untuk menambahkan garis regresi, yaitu yang pertama adalah menggunakan sns.regplot() seperti di bawah ini.\n\nsns.regplot(data=insurance_df.query('smoker == \"yes\"'), x='bmi', y='charges') # axes 1\nsns.regplot(data=insurance_df.query('smoker == \"no\"'), x='bmi', y='charges') # axes 2\nplt.show()\n\n\n\n\nPerhatikan bahwa sns.regplot() dipanggil dua kali karena fungsi tersebut tidak memiliki parameter hue.\nUntuk mempermudah, kita dapat menggunakan cara kedua, yaitu menggunakan sns.lmplot(). Cara kerja sns.lmplot() yaitu menggabungkan dua (atau lebih) sns.regplot() dalam satu figur.\n\nsns.lmplot(data=insurance_df, x='bmi', y='charges', hue='smoker')\nplt.show()\n\n\n\n\n\n\n\nBox plot dan swarm plot biasa digunakan untuk melihat keterkaitan antara data kategorik dan data numerik. Swarm plot biasa disebut sebagai “categorical scatter plot”, karena plot yang dihasilkan mirip seperti scatter plot, tetapi untuk data kategorik.\nUntuk menggunakan box plot pada seaborn kita dapat menggunakan sns.boxplot().\nUntuk menggunakan swarm plot pada seaborn kita dapat menggunakan sns.swarmplot().\nMisal, kita ingin melihat keterkaitan antara fitur smoker dan charges menggunakan swarm plot. Maka, kita dapat menggunakan code seperti di bawah ini.\n\nplt.subplots(figsize=(10,6))\n\nsns.swarmplot(data=insurance_df, x='smoker', y='charges', size=3)\nplt.show()\n\n\n\n\nBerdasarkan swarm plot di atas, terlihat bahwa perokok cenderung memiliki biaya asuransi yang lebih tinggi dibandingkan yang bukan perokok. Selain itu, semakin lebar “swarm” pada suatu kategori berarti semakin banyak seseorang dengan nilai charges tersebut.\nApabila kita ingin menggunakan box plot, maka dapat digunakan code seperti di bawah ini.\n\nsns.boxplot(data=insurance_df, x='smoker', y='charges')\nplt.show()\n\n\n\n\nPada box plot, terdapat dua istilah yang umum digunakan, yaitu “box” dan “whiskers”. Pada box plot di atas, “box” merupakan persegi panjang berwarna biru dan orange. Garis di tengah box merupakan nilai mediannya, serta garis bawah dan garis atas box merupakan kuartil bawah (Q1) dan kuartil atas (Q3) secara berurutan. “Whiskers” adalah garis yang merupakan perpanjangan dari box. Ujung dari whiskers atas adalah Q3 + (1.5 x IQR) data, sedangkan ujung whiskers bawah adalah Q1 - (1.5 x IQR) data.\nTitik di luar box dan whiskers tersebut adalah titik yang biasa dijadikan sebagai outlier (penentuan outlier diserahkan ke diri masing-masing, apakah hanya dengan melihat box plot atau dengan menggunakan metode lain, tetapi untuk mempermudah dapat menggunakan box plot).\n\n\n\nSelain box plot dan swarm plot, kita juga dapat melihat persebaran data menggunakan histogram dan density plot. Histogram biasa digunakan untuk melihat persebaran data secara diskrit, sedangkan density plot untuk melihat persebaran data secara kontinu.\nUntuk membuat histogram pada seaborn, kita dapat menggunakan sns.histplot().\nUntuk membuat density plot pada seaborn, kita dapat menggunakan sns.kdeplot().\nMisal, kita ingin melihat persebaran dari fitur charges pada insurance_df. Maka dapat digunakan code seperti di bawah.\n\nplt.subplots(figsize=(12,6))\n\nsns.histplot(data=insurance_df, x='charges')\nplt.show()\n\n\n\n\nBerdasarkan histogram di atas, terlihat bahwa distribusi charges cenderung “skew” atau miring ke kanan. “Skewness” atau tingkat kecondongan merupakan aspek yang penting untuk diperhatikan ketika kita ingin membuat model Machine Learning.\nSeperti scatter plot, kita juga dapat menentukan pewarnaan histogram berdasarkan fitur lainnya dengan menggunakan parameter hue seperti di bawah ini/\n\nplt.subplots(figsize=(12,6))\nsns.histplot(data=insurance_df, x='charges', hue='smoker')\nplt.show()\n\n\n\n\nJika ingin membuat density plot dari fitur charges, kita dapat menggunakan kode seperti di bawah ini. Parameter shade berfungsi untuk memberikan warna di bawah kurva.\n\nplt.subplots(figsize=(12,6))\nsns.kdeplot(data=insurance_df, x='charges', shade=True)\nplt.show()\n\n\n\n\nsns.kdeplot() juga dapat menggunakan parameter hue.\n\nplt.subplots(figsize=(12,6))\nsns.kdeplot(data=insurance_df, x='charges',\n            hue='smoker', shade=True)\nplt.show()\n\n\n\n\nApabila kita ingin menggabungkan histogram dan density plot dalam satu figur, kita dapat menggunakan sns.histplot() dengan parameter kde=True.\n\nplt.subplots(figsize=(12,6))\nsns.histplot(data=insurance_df, x='charges', hue='smoker', kde=True)\nplt.show()\n\n\n\n\n\n\n\nPada seaborn, kita juga dapat membuat dua plot yang berbeda dari dua fitur dalam satu figur yang sama menggunakan sns.jointplot().\nJenis plot yang dihasilkan dapat diatur pada parameter kind. Pilihan jenis kind yang disediakan dapat dilihat pada: https://seaborn.pydata.org/generated/seaborn.jointplot.html\n\nsns.jointplot(data=insurance_df, x='charges', y='bmi', hue='smoker', kind=\"scatter\")\n\nplt.show()\n\n\n\n\n\nsns.jointplot(data=insurance_df, x='charges', y='bmi', hue='smoker', kind=\"hist\")\n\nplt.show()\n\n\n\n\n\nsns.jointplot(data=insurance_df, x='charges', y='bmi', hue='smoker', kind=\"kde\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nsource: https://www.kaggle.com/code/alexisbcook/choosing-plot-types-and-custom-styles"
  },
  {
    "objectID": "main-module/week-03.html",
    "href": "main-module/week-03.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "import library yang dibutuhkan terlebih dahulu untuk pengolahan dan visualisasi data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\nUpload dataset yang akan digunakan dan observasi click disini\n\nsalary =  pd.read_csv('Salary_dataset.csv')\nsalary\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      YearsExperience\n      Salary\n    \n  \n  \n    \n      0\n      0\n      1.2\n      39344.0\n    \n    \n      1\n      1\n      1.4\n      46206.0\n    \n    \n      2\n      2\n      1.6\n      37732.0\n    \n    \n      3\n      3\n      2.1\n      43526.0\n    \n    \n      4\n      4\n      2.3\n      39892.0\n    \n    \n      5\n      5\n      3.0\n      56643.0\n    \n    \n      6\n      6\n      3.1\n      60151.0\n    \n    \n      7\n      7\n      3.3\n      54446.0\n    \n    \n      8\n      8\n      3.3\n      64446.0\n    \n    \n      9\n      9\n      3.8\n      57190.0\n    \n    \n      10\n      10\n      4.0\n      63219.0\n    \n    \n      11\n      11\n      4.1\n      55795.0\n    \n    \n      12\n      12\n      4.1\n      56958.0\n    \n    \n      13\n      13\n      4.2\n      57082.0\n    \n    \n      14\n      14\n      4.6\n      61112.0\n    \n    \n      15\n      15\n      5.0\n      67939.0\n    \n    \n      16\n      16\n      5.2\n      66030.0\n    \n    \n      17\n      17\n      5.4\n      83089.0\n    \n    \n      18\n      18\n      6.0\n      81364.0\n    \n    \n      19\n      19\n      6.1\n      93941.0\n    \n    \n      20\n      20\n      6.9\n      91739.0\n    \n    \n      21\n      21\n      7.2\n      98274.0\n    \n    \n      22\n      22\n      8.0\n      101303.0\n    \n    \n      23\n      23\n      8.3\n      113813.0\n    \n    \n      24\n      24\n      8.8\n      109432.0\n    \n    \n      25\n      25\n      9.1\n      105583.0\n    \n    \n      26\n      26\n      9.6\n      116970.0\n    \n    \n      27\n      27\n      9.7\n      112636.0\n    \n    \n      28\n      28\n      10.4\n      122392.0\n    \n    \n      29\n      29\n      10.6\n      121873.0\n    \n  \n\n\n\n\n\nsalary.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30 entries, 0 to 29\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       30 non-null     int64  \n 1   YearsExperience  30 non-null     float64\n 2   Salary           30 non-null     float64\ndtypes: float64(2), int64(1)\nmemory usage: 848.0 bytes\n\n\n\n\n\nMelihat jumlah data null pada dataset\n\nsalary.isna().sum()\n\nUnnamed: 0         0\nYearsExperience    0\nSalary             0\ndtype: int64\n\n\nMelihat jumlah data duplikat pada dataset\n\nsalary.duplicated().sum()\n\n0\n\n\nMenghapus kolom ‘Unnamed :0’ dari DataFrame secara permanen\n\nsalary.drop('Unnamed: 0', axis=1, inplace=True)\n\n\nsalary\n\n\n\n\n\n  \n    \n      \n      YearsExperience\n      Salary\n    \n  \n  \n    \n      0\n      1.2\n      39344.0\n    \n    \n      1\n      1.4\n      46206.0\n    \n    \n      2\n      1.6\n      37732.0\n    \n    \n      3\n      2.1\n      43526.0\n    \n    \n      4\n      2.3\n      39892.0\n    \n    \n      5\n      3.0\n      56643.0\n    \n    \n      6\n      3.1\n      60151.0\n    \n    \n      7\n      3.3\n      54446.0\n    \n    \n      8\n      3.3\n      64446.0\n    \n    \n      9\n      3.8\n      57190.0\n    \n    \n      10\n      4.0\n      63219.0\n    \n    \n      11\n      4.1\n      55795.0\n    \n    \n      12\n      4.1\n      56958.0\n    \n    \n      13\n      4.2\n      57082.0\n    \n    \n      14\n      4.6\n      61112.0\n    \n    \n      15\n      5.0\n      67939.0\n    \n    \n      16\n      5.2\n      66030.0\n    \n    \n      17\n      5.4\n      83089.0\n    \n    \n      18\n      6.0\n      81364.0\n    \n    \n      19\n      6.1\n      93941.0\n    \n    \n      20\n      6.9\n      91739.0\n    \n    \n      21\n      7.2\n      98274.0\n    \n    \n      22\n      8.0\n      101303.0\n    \n    \n      23\n      8.3\n      113813.0\n    \n    \n      24\n      8.8\n      109432.0\n    \n    \n      25\n      9.1\n      105583.0\n    \n    \n      26\n      9.6\n      116970.0\n    \n    \n      27\n      9.7\n      112636.0\n    \n    \n      28\n      10.4\n      122392.0\n    \n    \n      29\n      10.6\n      121873.0\n    \n  \n\n\n\n\n\n\n\nMengubah setiap nilai di kolom Salary dan mengubah nama kolomnya di DataFrame secara permanen\n\nsalary['Salary'] = salary['Salary']/1000\nsalary.rename(columns={'Salary' : 'Salary (1000 $)'}, inplace=True)\n\nMelihat statistik deskriptif dari DataFrame\n\nsalary.describe()\n\n\n\n\n\n  \n    \n      \n      YearsExperience\n      Salary (1000 $)\n    \n  \n  \n    \n      count\n      30.000000\n      30.00000\n    \n    \n      mean\n      5.413333\n      76.00400\n    \n    \n      std\n      2.837888\n      27.41443\n    \n    \n      min\n      1.200000\n      37.73200\n    \n    \n      25%\n      3.300000\n      56.72175\n    \n    \n      50%\n      4.800000\n      65.23800\n    \n    \n      75%\n      7.800000\n      100.54575\n    \n    \n      max\n      10.600000\n      122.39200\n    \n  \n\n\n\n\n\nplt.scatter(salary['YearsExperience'],salary['Salary (1000 $)'])\nplt.plot(salary['YearsExperience'],salary['Salary (1000 $)'])\nplt.xlabel('Year Experience')\nplt.ylabel('Salary (1000 $)')\nplt.show()\n\n\n\n\n\nfig, (ax_box, ax_hist) = plt.subplots(2, 1, figsize=(6, 6), sharex='col',\n                                      gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(data=salary, x='Salary (1000 $)', ax=ax_box, color='crimson')\nsns.histplot(data=salary, x='Salary (1000 $)', ax=ax_hist, binwidth=10.)\nsns.rugplot(data=salary, x='Salary (1000 $)', ax=ax_hist, height=0.05, color='gold', lw=2.)\nplt.tight_layout()\n\n\n\n\n\nfig, (ax_box, ax_hist) = plt.subplots(2, 1, figsize=(6, 6), sharex='col',\n                                      gridspec_kw={\"height_ratios\": (.15, .85)})\n\nsns.boxplot(data=salary, x='YearsExperience', ax=ax_box, color='crimson')\nsns.histplot(data=salary, x='YearsExperience', ax=ax_hist, binwidth=1.)\nsns.rugplot(data=salary, x='YearsExperience', ax=ax_hist, height=0.05, color='gold', lw=2.)\nplt.tight_layout()\n\n\n\n\n\ncorr = salary.corr()\nsns.heatmap(corr, vmin=-1, center=0, vmax=1, annot=True)\nplt.show()\n\n\n\n\n\nplt.subplots(figsize=(6,6))\n\nsns.regplot(data = salary, x='YearsExperience', y='Salary (1000 $)', color='k', marker='+')\nplt.show()\n\n\n\n\n\n\n\nKarena pada dataset ini, fitur yang ada hanya 2, tidak ada masalah dan data sudah rapi, maka untuk step feature engineering akan skip dan lanjut ke tahap modelling.\n\n\n\n\nX = salary[['YearsExperience']]\ny = salary[['Salary (1000 $)']]\n\n\nX\n\n\n\n\n\n  \n    \n      \n      YearsExperience\n    \n  \n  \n    \n      0\n      1.2\n    \n    \n      1\n      1.4\n    \n    \n      2\n      1.6\n    \n    \n      3\n      2.1\n    \n    \n      4\n      2.3\n    \n    \n      5\n      3.0\n    \n    \n      6\n      3.1\n    \n    \n      7\n      3.3\n    \n    \n      8\n      3.3\n    \n    \n      9\n      3.8\n    \n    \n      10\n      4.0\n    \n    \n      11\n      4.1\n    \n    \n      12\n      4.1\n    \n    \n      13\n      4.2\n    \n    \n      14\n      4.6\n    \n    \n      15\n      5.0\n    \n    \n      16\n      5.2\n    \n    \n      17\n      5.4\n    \n    \n      18\n      6.0\n    \n    \n      19\n      6.1\n    \n    \n      20\n      6.9\n    \n    \n      21\n      7.2\n    \n    \n      22\n      8.0\n    \n    \n      23\n      8.3\n    \n    \n      24\n      8.8\n    \n    \n      25\n      9.1\n    \n    \n      26\n      9.6\n    \n    \n      27\n      9.7\n    \n    \n      28\n      10.4\n    \n    \n      29\n      10.6\n    \n  \n\n\n\n\n\ny\n\n\n\n\n\n  \n    \n      \n      Salary (1000 $)\n    \n  \n  \n    \n      0\n      39.344\n    \n    \n      1\n      46.206\n    \n    \n      2\n      37.732\n    \n    \n      3\n      43.526\n    \n    \n      4\n      39.892\n    \n    \n      5\n      56.643\n    \n    \n      6\n      60.151\n    \n    \n      7\n      54.446\n    \n    \n      8\n      64.446\n    \n    \n      9\n      57.190\n    \n    \n      10\n      63.219\n    \n    \n      11\n      55.795\n    \n    \n      12\n      56.958\n    \n    \n      13\n      57.082\n    \n    \n      14\n      61.112\n    \n    \n      15\n      67.939\n    \n    \n      16\n      66.030\n    \n    \n      17\n      83.089\n    \n    \n      18\n      81.364\n    \n    \n      19\n      93.941\n    \n    \n      20\n      91.739\n    \n    \n      21\n      98.274\n    \n    \n      22\n      101.303\n    \n    \n      23\n      113.813\n    \n    \n      24\n      109.432\n    \n    \n      25\n      105.583\n    \n    \n      26\n      116.970\n    \n    \n      27\n      112.636\n    \n    \n      28\n      122.392\n    \n    \n      29\n      121.873\n    \n  \n\n\n\n\nSplit dataset menjadi data train dan data test dengan komposisi pembagian yang sering digunakan\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((24, 1), (6, 1), (24, 1), (6, 1))\n\n\nImport terlebih dahulu package yang akan digunakan untuk modelling\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred = lr.predict(X_test)\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(mean_squared_error(y_pred,y_test))\nprint(r2_score(y_pred,y_test))\n\n49.830096855908344\n0.8961838737587329\n\n\n \nDimana:\n\\(n\\) : jumlah data\n\\(Y_i\\) : nilai actual\n\\(\\hat{Y}_{i}\\): nilai predict\n\\(RSS\\) : sum of squared residuals\n\\(TSS\\) : total sum of squares\n\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n\n[[115.79121011 112.636     ]\n [ 71.49927809  67.939     ]\n [102.59786866 113.813     ]\n [ 75.26880422  83.089     ]\n [ 55.47879205  64.446     ]\n [ 60.19069971  57.19      ]]\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n\n\nheart = pd.read_csv('heart.csv')\nheart\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      cp\n      trtbps\n      chol\n      fbs\n      restecg\n      thalachh\n      exng\n      oldpeak\n      slp\n      caa\n      thall\n      output\n    \n  \n  \n    \n      0\n      63\n      1\n      3\n      145\n      233\n      1\n      0\n      150\n      0\n      2.3\n      0\n      0\n      1\n      1\n    \n    \n      1\n      37\n      1\n      2\n      130\n      250\n      0\n      1\n      187\n      0\n      3.5\n      0\n      0\n      2\n      1\n    \n    \n      2\n      41\n      0\n      1\n      130\n      204\n      0\n      0\n      172\n      0\n      1.4\n      2\n      0\n      2\n      1\n    \n    \n      3\n      56\n      1\n      1\n      120\n      236\n      0\n      1\n      178\n      0\n      0.8\n      2\n      0\n      2\n      1\n    \n    \n      4\n      57\n      0\n      0\n      120\n      354\n      0\n      1\n      163\n      1\n      0.6\n      2\n      0\n      2\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      298\n      57\n      0\n      0\n      140\n      241\n      0\n      1\n      123\n      1\n      0.2\n      1\n      0\n      3\n      0\n    \n    \n      299\n      45\n      1\n      3\n      110\n      264\n      0\n      1\n      132\n      0\n      1.2\n      1\n      0\n      3\n      0\n    \n    \n      300\n      68\n      1\n      0\n      144\n      193\n      1\n      1\n      141\n      0\n      3.4\n      1\n      2\n      3\n      0\n    \n    \n      301\n      57\n      1\n      0\n      130\n      131\n      0\n      1\n      115\n      1\n      1.2\n      1\n      1\n      3\n      0\n    \n    \n      302\n      57\n      0\n      1\n      130\n      236\n      0\n      0\n      174\n      0\n      0.0\n      1\n      1\n      2\n      0\n    \n  \n\n303 rows × 14 columns\n\n\n\n\n# Membaca .txt tentang kolom - kolom dataset yang diberikan pada soal\nwith open('about dataset.txt', 'r') as f:\n  print(f.read())\n\nAbout datasets\n1. age - age in years \n2. sex - sex (1 = male; 0 = female) \n3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic) \n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) \n5. chol - serum cholestoral in mg/dl \n6. fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false) \n7. restecg - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy) \n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n11. slope - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping) \n12. ca - number of major vessels (0-3) colored by flourosopy \n13. thal - 2 = normal; 1 = fixed defect; 3 = reversable defect \n14. output - the predicted attribute - diagnosis of heart disease (0 = less chance of heart attack, 1 = higher chance of heart attack)\n\n\n\n\nheart.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trtbps    303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalachh  303 non-null    int64  \n 8   exng      303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slp       303 non-null    int64  \n 11  caa       303 non-null    int64  \n 12  thall     303 non-null    int64  \n 13  output    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n\n\n\nheart.output.value_counts()\n\n1    165\n0    138\nName: output, dtype: int64\n\n\n\n\n\n\nheart.describe()\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      cp\n      trtbps\n      chol\n      fbs\n      restecg\n      thalachh\n      exng\n      oldpeak\n      slp\n      caa\n      thall\n      output\n    \n  \n  \n    \n      count\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n      303.000000\n    \n    \n      mean\n      54.366337\n      0.683168\n      0.966997\n      131.623762\n      246.264026\n      0.148515\n      0.528053\n      149.646865\n      0.326733\n      1.039604\n      1.399340\n      0.729373\n      2.313531\n      0.544554\n    \n    \n      std\n      9.082101\n      0.466011\n      1.032052\n      17.538143\n      51.830751\n      0.356198\n      0.525860\n      22.905161\n      0.469794\n      1.161075\n      0.616226\n      1.022606\n      0.612277\n      0.498835\n    \n    \n      min\n      29.000000\n      0.000000\n      0.000000\n      94.000000\n      126.000000\n      0.000000\n      0.000000\n      71.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      47.500000\n      0.000000\n      0.000000\n      120.000000\n      211.000000\n      0.000000\n      0.000000\n      133.500000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      2.000000\n      0.000000\n    \n    \n      50%\n      55.000000\n      1.000000\n      1.000000\n      130.000000\n      240.000000\n      0.000000\n      1.000000\n      153.000000\n      0.000000\n      0.800000\n      1.000000\n      0.000000\n      2.000000\n      1.000000\n    \n    \n      75%\n      61.000000\n      1.000000\n      2.000000\n      140.000000\n      274.500000\n      0.000000\n      1.000000\n      166.000000\n      1.000000\n      1.600000\n      2.000000\n      1.000000\n      3.000000\n      1.000000\n    \n    \n      max\n      77.000000\n      1.000000\n      3.000000\n      200.000000\n      564.000000\n      1.000000\n      2.000000\n      202.000000\n      1.000000\n      6.200000\n      2.000000\n      4.000000\n      3.000000\n      1.000000\n    \n  \n\n\n\n\n\npd.plotting.scatter_matrix(heart[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']], figsize=(15,12)) # plot data yang numerik dan kontinu\nplt.show()\n\n\n\n\nPlot diatas saya ingin melihat korelasi secara kasar antara fitur - fitur yang numerik dan kontinu, melalui scatter plot, serta range nilai datanya melalui histogramnya.\nMelalui scatter plot dapat kita lihat bahwa kita belum bisa menyimpulkan korelasi antara fitur - fitur, karena persebarannya sebagian besar sangat acak. Melalui histogram dapat dilihat bahwa range nilainya cukup berjauhan (oldpeak 0 sampai 6, sedangkan chol 100 sampai 500+), sehingga perlu dilakukan standarisasi pada data numerik nantinya dengan StandardScaler\n\ncorr = heart.corr()\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, vmin=-1, center=0, vmax=1, annot=True)\nplt.show()\n\n\n\n\n\n\n\n\nX = heart.drop('output',axis=1).copy()\ny = heart.iloc[:,[-1]]\n\n\nX\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      cp\n      trtbps\n      chol\n      fbs\n      restecg\n      thalachh\n      exng\n      oldpeak\n      slp\n      caa\n      thall\n    \n  \n  \n    \n      0\n      63\n      1\n      3\n      145\n      233\n      1\n      0\n      150\n      0\n      2.3\n      0\n      0\n      1\n    \n    \n      1\n      37\n      1\n      2\n      130\n      250\n      0\n      1\n      187\n      0\n      3.5\n      0\n      0\n      2\n    \n    \n      2\n      41\n      0\n      1\n      130\n      204\n      0\n      0\n      172\n      0\n      1.4\n      2\n      0\n      2\n    \n    \n      3\n      56\n      1\n      1\n      120\n      236\n      0\n      1\n      178\n      0\n      0.8\n      2\n      0\n      2\n    \n    \n      4\n      57\n      0\n      0\n      120\n      354\n      0\n      1\n      163\n      1\n      0.6\n      2\n      0\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      298\n      57\n      0\n      0\n      140\n      241\n      0\n      1\n      123\n      1\n      0.2\n      1\n      0\n      3\n    \n    \n      299\n      45\n      1\n      3\n      110\n      264\n      0\n      1\n      132\n      0\n      1.2\n      1\n      0\n      3\n    \n    \n      300\n      68\n      1\n      0\n      144\n      193\n      1\n      1\n      141\n      0\n      3.4\n      1\n      2\n      3\n    \n    \n      301\n      57\n      1\n      0\n      130\n      131\n      0\n      1\n      115\n      1\n      1.2\n      1\n      1\n      3\n    \n    \n      302\n      57\n      0\n      1\n      130\n      236\n      0\n      0\n      174\n      0\n      0.0\n      1\n      1\n      2\n    \n  \n\n303 rows × 13 columns\n\n\n\n\ny\n\n\n\n\n\n  \n    \n      \n      output\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n    \n      4\n      1\n    \n    \n      ...\n      ...\n    \n    \n      298\n      0\n    \n    \n      299\n      0\n    \n    \n      300\n      0\n    \n    \n      301\n      0\n    \n    \n      302\n      0\n    \n  \n\n303 rows × 1 columns\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nheart.columns\n\nIndex(['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh',\n       'exng', 'oldpeak', 'slp', 'caa', 'thall', 'output'],\n      dtype='object')\n\n\n\nsc = StandardScaler()\ncol = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nX_train.loc[:,col] = sc.fit_transform(X_train.loc[:,col])\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      cp\n      trtbps\n      chol\n      fbs\n      restecg\n      thalachh\n      exng\n      oldpeak\n      slp\n      caa\n      thall\n    \n  \n  \n    \n      132\n      -1.356798\n      1\n      1\n      -0.616856\n      0.914034\n      0\n      1\n      0.532781\n      0\n      -0.920864\n      2\n      0\n      2\n    \n    \n      202\n      0.385086\n      1\n      0\n      1.169491\n      0.439527\n      0\n      0\n      -1.753582\n      1\n      -0.193787\n      2\n      0\n      3\n    \n    \n      196\n      -0.921327\n      1\n      2\n      1.169491\n      -0.300704\n      0\n      1\n      -0.139679\n      0\n      2.350982\n      1\n      0\n      2\n    \n    \n      75\n      0.058483\n      0\n      1\n      0.276318\n      0.059921\n      0\n      0\n      0.487950\n      0\n      0.351521\n      1\n      0\n      2\n    \n    \n      176\n      0.602822\n      1\n      0\n      -0.795490\n      -0.319684\n      1\n      1\n      0.443119\n      1\n      0.351521\n      2\n      2\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      188\n      -0.485856\n      1\n      2\n      0.574042\n      -0.262744\n      0\n      1\n      0.577611\n      0\n      -0.375556\n      1\n      1\n      3\n    \n    \n      71\n      -0.376988\n      1\n      2\n      -2.165023\n      -0.376625\n      0\n      1\n      0.174136\n      1\n      -0.920864\n      2\n      1\n      3\n    \n    \n      106\n      1.582631\n      1\n      3\n      1.764940\n      -0.243763\n      1\n      0\n      -0.856969\n      0\n      -0.829979\n      1\n      1\n      2\n    \n    \n      270\n      -0.921327\n      1\n      0\n      -0.616856\n      0.040941\n      0\n      0\n      -0.274171\n      0\n      -0.193787\n      2\n      0\n      3\n    \n    \n      102\n      0.929425\n      0\n      1\n      0.574042\n      -0.983994\n      0\n      1\n      1.294902\n      0\n      -0.920864\n      2\n      2\n      2\n    \n  \n\n242 rows × 13 columns\n\n\n\n\nX_test.loc[:,col] = sc.transform(X_test.loc[:,col])\nX_test\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      cp\n      trtbps\n      chol\n      fbs\n      restecg\n      thalachh\n      exng\n      oldpeak\n      slp\n      caa\n      thall\n    \n  \n  \n    \n      179\n      0.276218\n      1\n      0\n      1.169491\n      0.553408\n      0\n      0\n      -1.708752\n      1\n      -0.375556\n      1\n      1\n      1\n    \n    \n      228\n      0.493954\n      1\n      3\n      2.360389\n      0.781172\n      0\n      0\n      0.398289\n      0\n      -0.739095\n      1\n      0\n      3\n    \n    \n      111\n      0.276218\n      1\n      2\n      1.169491\n      -2.293633\n      1\n      1\n      1.025918\n      0\n      -0.739095\n      2\n      1\n      3\n    \n    \n      246\n      0.167350\n      0\n      0\n      0.216773\n      3.077785\n      0\n      0\n      -0.005187\n      1\n      0.805944\n      1\n      2\n      3\n    \n    \n      60\n      1.800367\n      0\n      2\n      -1.212304\n      0.344625\n      1\n      0\n      -0.901800\n      0\n      -0.920864\n      2\n      1\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      249\n      1.582631\n      1\n      2\n      0.574042\n      0.135842\n      0\n      0\n      -0.184510\n      0\n      0.896828\n      1\n      3\n      3\n    \n    \n      104\n      -0.485856\n      1\n      2\n      -0.080952\n      -0.965014\n      0\n      1\n      0.577611\n      0\n      -0.920864\n      2\n      0\n      2\n    \n    \n      300\n      1.473764\n      1\n      0\n      0.812222\n      -1.021955\n      1\n      1\n      -0.408663\n      0\n      2.169213\n      1\n      2\n      3\n    \n    \n      193\n      0.602822\n      1\n      0\n      0.871767\n      0.667290\n      0\n      0\n      -0.363832\n      1\n      1.623905\n      1\n      2\n      3\n    \n    \n      184\n      -0.485856\n      1\n      0\n      1.169491\n      -0.072941\n      0\n      0\n      -0.991461\n      0\n      1.442136\n      1\n      0\n      3\n    \n  \n\n61 rows × 13 columns\n\n\n\n\n\n\n\nlog_regr = LogisticRegression()\nsvc = SVC()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\n\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# melakukan cross validation pada masing-masing metode\nlr_score = cross_val_score(log_regr, X_train, y_train, cv=kfold, scoring='f1').mean()\nsvc_score = cross_val_score(svc, X_train, y_train, cv=kfold, scoring='f1').mean()\ndt_score = cross_val_score(dt, X_train, y_train, cv=kfold, scoring='f1').mean()\nrf_score = cross_val_score(rf, X_train, y_train, cv=kfold, scoring='f1').mean()\n\n\nfor i in [lr_score, svc_score, dt_score, rf_score]:\n    print(i)\n\n0.838821143443002\n0.8530945548368415\n0.7278904812545365\n0.8365591551305837\n\n\n\n\n\n\nparams = {'C':[0.01,0.05,0.1,0.7,0.5,1,5,10,50,100],     # hyperparameter yang akan dievaluasi untuk SVC\n             'kernel':['poly','rbf']}\n\ngrid_search = GridSearchCV(svc, params, cv=kfold, scoring='f1')\ngrid_search.fit(X_train,y_train)\n\n\ngrid_search.best_params_, grid_search.cv_results_['mean_test_score'].max()\n\n({'C': 0.7, 'kernel': 'rbf'}, 0.8596614105205573)\n\n\n\nmodel = grid_search.best_estimator_\nmodel.fit(X_train,y_train)\n\nC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nSVC(C=0.7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=0.7)\n\n\n\ny_pred = model.predict(X_test)\ny_pred\n\narray([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], dtype=int64)\n\n\n\n\n\n\nf1_score(y_test,y_pred)\n\n0.8923076923076922\n\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\ndef evaluation_parametrics(name,y_val, y_pred):\n    \n    print(\"\\n------------------------{}------------------------\\n\".format(name))\n\n    cm_test = confusion_matrix(y_val, y_pred)\n    t1 = ConfusionMatrixDisplay(cm_test)    \n    print(\"\\nClassification Report for Data Test\\n\")\n    print(classification_report(y_val, y_pred))   \n    print(\"--------------------------------------------------------------------------\")\n\n    t1.plot()\n\n\nevaluation_parametrics(\"Machine Learning - Classification\", y_test, y_pred)\n\n\n------------------------Machine Learning - Classification------------------------\n\n\nClassification Report for Data Test\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.86      0.88        29\n           1       0.88      0.91      0.89        32\n\n    accuracy                           0.89        61\n   macro avg       0.89      0.88      0.88        61\nweighted avg       0.89      0.89      0.89        61\n\n--------------------------------------------------------------------------\n\n\n\n\n\n\n\n\nimage.png\n\n\nPerbandingan data actual dan data prediksi\n\nprint(np.concatenate((y_test.values.reshape(len(y_test),1),y_pred.reshape(len(y_pred),1)),1))\n\n[[0 0]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 1]\n [0 1]\n [1 1]\n [0 0]\n [1 1]\n [1 0]\n [0 0]\n [0 0]\n [1 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]]\n\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\nresult = permutation_importance(model, X_test, y_test, n_repeats=10,\n                                scoring='f1', random_state=42)\n\n\nresult_sorted = []\ncolumns_sorted = []\n\nfor res, col in sorted(zip(result.importances_mean, X_test.columns.values), reverse=True):\n  result_sorted.append(res)\n  columns_sorted.append(col)\n\nsns.barplot(x = result_sorted, y = columns_sorted)\nplt.show()\n\n\n\n\n\n\n\nSimpan model ke dalam file dan model siap digunakan untuk predict\n\nimport joblib\njoblib.dump(model,'model_SVC.pkl')\n\n['model_SVC.pkl']"
  },
  {
    "objectID": "main-module/week-04.html",
    "href": "main-module/week-04.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Clustering adalah metode untuk membagi populasi atau titik data ke dalam sejumlah kelompok sedemikian rupa sehingga titik data dalam kelompok yang sama lebih mirip dengan titik data lain dalam kelompok yang sama dan berbeda dengan titik data dalam kelompok lain. Pada dasarnya, ini adalah kumpulan objek berdasarkan kesamaan dan ketidaksamaan di antara mereka.\nsource: https://www.geeksforgeeks.org/clustering-in-machine-learning/\nDataset: https://drive.google.com/file/d/1QWrWOYx2cZyEfYLe652Aj8IMm8mlkMy9/view?usp=sharing\n\n\nK-means clustering adalah algoritma unsupervised learning yang mengelompokkan dataset yang belum dilabel ke dalam kluster yang berbeda berdasarkan kesamaan tertentu\\(^{[1][2][3]}\\). K-means clustering membutuhkan nilai k yang menandakan jumlah kluster yang akan dibentuk\\(^{[1][4]}\\). K-means clustering berusaha untuk meminimalisasi variasi antar kluster dan memaksimalisasi variasi antar kluster\\(^{[2][5]}\\). K-means clustering menggunakan rata-rata dan jarak antara data dan centroid untuk menentukan kluster\\(^{[2][5]}\\). K-means clustering bekerja dengan baik jika kluster memiliki bentuk bola\\(^{[3]}\\).\nsource:\n[1] https://www.trivusi.web.id/2022/06/algoritma-kmeans-clustering.html\n[2] https://raharja.ac.id/2020/04/19/k-means-clustering/\n[3] https://ichi.pro/id/k-means-clustering-algoritma-aplikasi-metode-evaluasi-dan-kelemahan-186933724886154\n[4] https://dqlab.id/k-means-clustering-salah-satu-contoh-teknik-analisis-data-populer\n[5] https://sis.binus.ac.id/2022/01/31/clustering-algoritma-k-means/.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n\ndataset = pd.read_csv('Mall_Customers.csv')\ndataset\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      CustomerID\n      Genre\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      1\n      Male\n      19\n      15\n      39\n    \n    \n      1\n      2\n      Male\n      21\n      15\n      81\n    \n    \n      2\n      3\n      Female\n      20\n      16\n      6\n    \n    \n      3\n      4\n      Female\n      23\n      16\n      77\n    \n    \n      4\n      5\n      Female\n      31\n      17\n      40\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      195\n      196\n      Female\n      35\n      120\n      79\n    \n    \n      196\n      197\n      Female\n      45\n      126\n      28\n    \n    \n      197\n      198\n      Male\n      32\n      126\n      74\n    \n    \n      198\n      199\n      Male\n      32\n      137\n      18\n    \n    \n      199\n      200\n      Male\n      30\n      137\n      83\n    \n  \n\n200 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nX = dataset.iloc[:, [3, 4]].values\nX\n\narray([[ 15,  39],\n       [ 15,  81],\n       [ 16,   6],\n       [ 16,  77],\n       [ 17,  40],\n       [ 17,  76],\n       [ 18,   6],\n       [ 18,  94],\n       [ 19,   3],\n       [ 19,  72],\n       [ 19,  14],\n       [ 19,  99],\n       [ 20,  15],\n       [ 20,  77],\n       [ 20,  13],\n       [ 20,  79],\n       [ 21,  35],\n       [ 21,  66],\n       [ 23,  29],\n       [ 23,  98],\n       [ 24,  35],\n       [ 24,  73],\n       [ 25,   5],\n       [ 25,  73],\n       [ 28,  14],\n       [ 28,  82],\n       [ 28,  32],\n       [ 28,  61],\n       [ 29,  31],\n       [ 29,  87],\n       [ 30,   4],\n       [ 30,  73],\n       [ 33,   4],\n       [ 33,  92],\n       [ 33,  14],\n       [ 33,  81],\n       [ 34,  17],\n       [ 34,  73],\n       [ 37,  26],\n       [ 37,  75],\n       [ 38,  35],\n       [ 38,  92],\n       [ 39,  36],\n       [ 39,  61],\n       [ 39,  28],\n       [ 39,  65],\n       [ 40,  55],\n       [ 40,  47],\n       [ 40,  42],\n       [ 40,  42],\n       [ 42,  52],\n       [ 42,  60],\n       [ 43,  54],\n       [ 43,  60],\n       [ 43,  45],\n       [ 43,  41],\n       [ 44,  50],\n       [ 44,  46],\n       [ 46,  51],\n       [ 46,  46],\n       [ 46,  56],\n       [ 46,  55],\n       [ 47,  52],\n       [ 47,  59],\n       [ 48,  51],\n       [ 48,  59],\n       [ 48,  50],\n       [ 48,  48],\n       [ 48,  59],\n       [ 48,  47],\n       [ 49,  55],\n       [ 49,  42],\n       [ 50,  49],\n       [ 50,  56],\n       [ 54,  47],\n       [ 54,  54],\n       [ 54,  53],\n       [ 54,  48],\n       [ 54,  52],\n       [ 54,  42],\n       [ 54,  51],\n       [ 54,  55],\n       [ 54,  41],\n       [ 54,  44],\n       [ 54,  57],\n       [ 54,  46],\n       [ 57,  58],\n       [ 57,  55],\n       [ 58,  60],\n       [ 58,  46],\n       [ 59,  55],\n       [ 59,  41],\n       [ 60,  49],\n       [ 60,  40],\n       [ 60,  42],\n       [ 60,  52],\n       [ 60,  47],\n       [ 60,  50],\n       [ 61,  42],\n       [ 61,  49],\n       [ 62,  41],\n       [ 62,  48],\n       [ 62,  59],\n       [ 62,  55],\n       [ 62,  56],\n       [ 62,  42],\n       [ 63,  50],\n       [ 63,  46],\n       [ 63,  43],\n       [ 63,  48],\n       [ 63,  52],\n       [ 63,  54],\n       [ 64,  42],\n       [ 64,  46],\n       [ 65,  48],\n       [ 65,  50],\n       [ 65,  43],\n       [ 65,  59],\n       [ 67,  43],\n       [ 67,  57],\n       [ 67,  56],\n       [ 67,  40],\n       [ 69,  58],\n       [ 69,  91],\n       [ 70,  29],\n       [ 70,  77],\n       [ 71,  35],\n       [ 71,  95],\n       [ 71,  11],\n       [ 71,  75],\n       [ 71,   9],\n       [ 71,  75],\n       [ 72,  34],\n       [ 72,  71],\n       [ 73,   5],\n       [ 73,  88],\n       [ 73,   7],\n       [ 73,  73],\n       [ 74,  10],\n       [ 74,  72],\n       [ 75,   5],\n       [ 75,  93],\n       [ 76,  40],\n       [ 76,  87],\n       [ 77,  12],\n       [ 77,  97],\n       [ 77,  36],\n       [ 77,  74],\n       [ 78,  22],\n       [ 78,  90],\n       [ 78,  17],\n       [ 78,  88],\n       [ 78,  20],\n       [ 78,  76],\n       [ 78,  16],\n       [ 78,  89],\n       [ 78,   1],\n       [ 78,  78],\n       [ 78,   1],\n       [ 78,  73],\n       [ 79,  35],\n       [ 79,  83],\n       [ 81,   5],\n       [ 81,  93],\n       [ 85,  26],\n       [ 85,  75],\n       [ 86,  20],\n       [ 86,  95],\n       [ 87,  27],\n       [ 87,  63],\n       [ 87,  13],\n       [ 87,  75],\n       [ 87,  10],\n       [ 87,  92],\n       [ 88,  13],\n       [ 88,  86],\n       [ 88,  15],\n       [ 88,  69],\n       [ 93,  14],\n       [ 93,  90],\n       [ 97,  32],\n       [ 97,  86],\n       [ 98,  15],\n       [ 98,  88],\n       [ 99,  39],\n       [ 99,  97],\n       [101,  24],\n       [101,  68],\n       [103,  17],\n       [103,  85],\n       [103,  23],\n       [103,  69],\n       [113,   8],\n       [113,  91],\n       [120,  16],\n       [120,  79],\n       [126,  28],\n       [126,  74],\n       [137,  18],\n       [137,  83]])\n\n\n\n\n\n\nhelp(KMeans)\n\nHelp on class KMeans in module sklearn.cluster._kmeans:\n\nclass KMeans(_BaseKMeans)\n |  KMeans(n_clusters=8, *, init='k-means++', n_init='warn', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n |  \n |  K-Means clustering.\n |  \n |  Read more in the :ref:`User Guide <k_means>`.\n |  \n |  Parameters\n |  ----------\n |  \n |  n_clusters : int, default=8\n |      The number of clusters to form as well as the number of\n |      centroids to generate.\n |  \n |  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n |      Method for initialization:\n |  \n |      'k-means++' : selects initial cluster centroids using sampling based on\n |      an empirical probability distribution of the points' contribution to the\n |      overall inertia. This technique speeds up convergence. The algorithm\n |      implemented is \"greedy k-means++\". It differs from the vanilla k-means++\n |      by making several trials at each sampling step and choosing the best centroid\n |      among them.\n |  \n |      'random': choose `n_clusters` observations (rows) at random from data\n |      for the initial centroids.\n |  \n |      If an array is passed, it should be of shape (n_clusters, n_features)\n |      and gives the initial centers.\n |  \n |      If a callable is passed, it should take arguments X, n_clusters and a\n |      random state and return an initialization.\n |  \n |  n_init : 'auto' or int, default=10\n |      Number of times the k-means algorithm is run with different centroid\n |      seeds. The final results is the best output of `n_init` consecutive runs\n |      in terms of inertia. Several runs are recommended for sparse\n |      high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n |  \n |      When `n_init='auto'`, the number of runs depends on the value of init:\n |      10 if using `init='random'`, 1 if using `init='k-means++'`.\n |  \n |      .. versionadded:: 1.2\n |         Added 'auto' option for `n_init`.\n |  \n |      .. versionchanged:: 1.4\n |         Default value for `n_init` will change from 10 to `'auto'` in version 1.4.\n |  \n |  max_iter : int, default=300\n |      Maximum number of iterations of the k-means algorithm for a\n |      single run.\n |  \n |  tol : float, default=1e-4\n |      Relative tolerance with regards to Frobenius norm of the difference\n |      in the cluster centers of two consecutive iterations to declare\n |      convergence.\n |  \n |  verbose : int, default=0\n |      Verbosity mode.\n |  \n |  random_state : int, RandomState instance or None, default=None\n |      Determines random number generation for centroid initialization. Use\n |      an int to make the randomness deterministic.\n |      See :term:`Glossary <random_state>`.\n |  \n |  copy_x : bool, default=True\n |      When pre-computing distances it is more numerically accurate to center\n |      the data first. If copy_x is True (default), then the original data is\n |      not modified. If False, the original data is modified, and put back\n |      before the function returns, but small numerical differences may be\n |      introduced by subtracting and then adding the data mean. Note that if\n |      the original data is not C-contiguous, a copy will be made even if\n |      copy_x is False. If the original data is sparse, but not in CSR format,\n |      a copy will be made even if copy_x is False.\n |  \n |  algorithm : {\"lloyd\", \"elkan\", \"auto\", \"full\"}, default=\"lloyd\"\n |      K-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\n |      The `\"elkan\"` variation can be more efficient on some datasets with\n |      well-defined clusters, by using the triangle inequality. However it's\n |      more memory intensive due to the allocation of an extra array of shape\n |      `(n_samples, n_clusters)`.\n |  \n |      `\"auto\"` and `\"full\"` are deprecated and they will be removed in\n |      Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`.\n |  \n |      .. versionchanged:: 0.18\n |          Added Elkan algorithm\n |  \n |      .. versionchanged:: 1.1\n |          Renamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\n |          Changed \"auto\" to use \"lloyd\" instead of \"elkan\".\n |  \n |  Attributes\n |  ----------\n |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n |      Coordinates of cluster centers. If the algorithm stops before fully\n |      converging (see ``tol`` and ``max_iter``), these will not be\n |      consistent with ``labels_``.\n |  \n |  labels_ : ndarray of shape (n_samples,)\n |      Labels of each point\n |  \n |  inertia_ : float\n |      Sum of squared distances of samples to their closest cluster center,\n |      weighted by the sample weights if provided.\n |  \n |  n_iter_ : int\n |      Number of iterations run.\n |  \n |  n_features_in_ : int\n |      Number of features seen during :term:`fit`.\n |  \n |      .. versionadded:: 0.24\n |  \n |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n |      Names of features seen during :term:`fit`. Defined only when `X`\n |      has feature names that are all strings.\n |  \n |      .. versionadded:: 1.0\n |  \n |  See Also\n |  --------\n |  MiniBatchKMeans : Alternative online implementation that does incremental\n |      updates of the centers positions using mini-batches.\n |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n |      probably much faster than the default batch implementation.\n |  \n |  Notes\n |  -----\n |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n |  \n |  The average complexity is given by O(k n T), where n is the number of\n |  samples and T is the number of iteration.\n |  \n |  The worst case complexity is given by O(n^(k+2/p)) with\n |  n = n_samples, p = n_features.\n |  Refer to :doi:`\"How slow is the k-means method?\" D. Arthur and S. Vassilvitskii -\n |  SoCG2006.<10.1145/1137856.1137880>` for more details.\n |  \n |  In practice, the k-means algorithm is very fast (one of the fastest\n |  clustering algorithms available), but it falls in local minima. That's why\n |  it can be useful to restart it several times.\n |  \n |  If the algorithm stops before fully converging (because of ``tol`` or\n |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n |  cluster. Also, the estimator will reassign ``labels_`` after the last\n |  iteration to make ``labels_`` consistent with ``predict`` on the training\n |  set.\n |  \n |  Examples\n |  --------\n |  \n |  >>> from sklearn.cluster import KMeans\n |  >>> import numpy as np\n |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n |  ...               [10, 2], [10, 4], [10, 0]])\n |  >>> kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n |  >>> kmeans.labels_\n |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n |  >>> kmeans.predict([[0, 0], [12, 3]])\n |  array([1, 0], dtype=int32)\n |  >>> kmeans.cluster_centers_\n |  array([[10.,  2.],\n |         [ 1.,  2.]])\n |  \n |  Method resolution order:\n |      KMeans\n |      _BaseKMeans\n |      sklearn.base.ClassNamePrefixFeaturesOutMixin\n |      sklearn.base.TransformerMixin\n |      sklearn.utils._set_output._SetOutputMixin\n |      sklearn.base.ClusterMixin\n |      sklearn.base.BaseEstimator\n |      abc.ABC\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, n_clusters=8, *, init='k-means++', n_init='warn', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y=None, sample_weight=None)\n |      Compute k-means clustering.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          Training instances to cluster. It must be noted that the data\n |          will be converted to C ordering, which will cause a memory\n |          copy if the given data is not C-contiguous.\n |          If a sparse matrix is passed, a copy will be made if it's not in\n |          CSR format.\n |      \n |      y : Ignored\n |          Not used, present here for API consistency by convention.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          The weights for each observation in X. If None, all observations\n |          are assigned equal weight.\n |      \n |          .. versionadded:: 0.20\n |      \n |      Returns\n |      -------\n |      self : object\n |          Fitted estimator.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from _BaseKMeans:\n |  \n |  fit_predict(self, X, y=None, sample_weight=None)\n |      Compute cluster centers and predict cluster index for each sample.\n |      \n |      Convenience method; equivalent to calling fit(X) followed by\n |      predict(X).\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          New data to transform.\n |      \n |      y : Ignored\n |          Not used, present here for API consistency by convention.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          The weights for each observation in X. If None, all observations\n |          are assigned equal weight.\n |      \n |      Returns\n |      -------\n |      labels : ndarray of shape (n_samples,)\n |          Index of the cluster each sample belongs to.\n |  \n |  fit_transform(self, X, y=None, sample_weight=None)\n |      Compute clustering and transform X to cluster-distance space.\n |      \n |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          New data to transform.\n |      \n |      y : Ignored\n |          Not used, present here for API consistency by convention.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          The weights for each observation in X. If None, all observations\n |          are assigned equal weight.\n |      \n |      Returns\n |      -------\n |      X_new : ndarray of shape (n_samples, n_clusters)\n |          X transformed in the new space.\n |  \n |  predict(self, X, sample_weight=None)\n |      Predict the closest cluster each sample in X belongs to.\n |      \n |      In the vector quantization literature, `cluster_centers_` is called\n |      the code book and each value returned by `predict` is the index of\n |      the closest code in the code book.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          New data to predict.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          The weights for each observation in X. If None, all observations\n |          are assigned equal weight.\n |      \n |      Returns\n |      -------\n |      labels : ndarray of shape (n_samples,)\n |          Index of the cluster each sample belongs to.\n |  \n |  score(self, X, y=None, sample_weight=None)\n |      Opposite of the value of X on the K-means objective.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          New data.\n |      \n |      y : Ignored\n |          Not used, present here for API consistency by convention.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          The weights for each observation in X. If None, all observations\n |          are assigned equal weight.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Opposite of the value of X on the K-means objective.\n |  \n |  transform(self, X)\n |      Transform X to a cluster-distance space.\n |      \n |      In the new space, each dimension is the distance to the cluster\n |      centers. Note that even if X is sparse, the array returned by\n |      `transform` will typically be dense.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          New data to transform.\n |      \n |      Returns\n |      -------\n |      X_new : ndarray of shape (n_samples, n_clusters)\n |          X transformed in the new space.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n |  \n |  get_feature_names_out(self, input_features=None)\n |      Get output feature names for transformation.\n |      \n |      The feature names out will prefixed by the lowercased class name. For\n |      example, if the transformer outputs 3 features, then the feature names\n |      out are: `[\"class_name0\", \"class_name1\", \"class_name2\"]`.\n |      \n |      Parameters\n |      ----------\n |      input_features : array-like of str or None, default=None\n |          Only used to validate feature names with the names seen in :meth:`fit`.\n |      \n |      Returns\n |      -------\n |      feature_names_out : ndarray of str objects\n |          Transformed feature names.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.ClassNamePrefixFeaturesOutMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n |  \n |  set_output(self, *, transform=None)\n |      Set output container.\n |      \n |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n |      for an example on how to use the API.\n |      \n |      Parameters\n |      ----------\n |      transform : {\"default\", \"pandas\"}, default=None\n |          Configure output of `transform` and `fit_transform`.\n |      \n |          - `\"default\"`: Default output format of a transformer\n |          - `\"pandas\"`: DataFrame output\n |          - `None`: Transform configuration is unchanged\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n |  \n |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from abc.ABCMeta\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n\n\n\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\nwcss\n\n[269981.28,\n 181363.59595959593,\n 106348.37306211122,\n 73679.78903948836,\n 44448.4554479337,\n 37233.814510710006,\n 30259.65720728547,\n 25011.839349156588,\n 21862.092672182895,\n 19672.072849014323]\n\n\nWithin-Cluster Sum-of-Squares criterion:\n\\(\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\\)\nsource:\n[1] https://scikit-learn.org/stable/modules/clustering.html#k-means\n[2] https://stats.stackexchange.com/questions/158210/k-means-why-minimizing-wcss-is-maximizing-distance-between-clusters\n\n\n\nNilai Silhouette mengukur seberapa mirip sebuah titik dengan klasternya sendiri (kohesi) dibandingkan dengan klaster lain (pemisahan).\nKisaran nilai Silhouette adalah antara +1 dan -1. Nilai yang tinggi diinginkan dan mengindikasikan bahwa titik tersebut ditempatkan pada klaster yang benar. Jika banyak titik yang memiliki nilai Silhouette negatif, hal ini dapat mengindikasikan bahwa kita telah membuat terlalu banyak atau terlalu sedikit cluster.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nsource: https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n\nimport sklearn.metrics as metrics\nfor i in range(2,11):\n  labels=KMeans(n_clusters=i,random_state=200).fit(X).labels_\n  print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"+str(metrics.silhouette_score(X,labels,metric=\"euclidean\",sample_size=1000,random_state=200)))\n\nSilhouette score for k(clusters) = 2 is 0.2968969162503008\nSilhouette score for k(clusters) = 3 is 0.46761358158775423\nSilhouette score for k(clusters) = 4 is 0.4931963109249047\nSilhouette score for k(clusters) = 5 is 0.553931997444648\nSilhouette score for k(clusters) = 6 is 0.5379675585622219\nSilhouette score for k(clusters) = 7 is 0.5367379891273258\nSilhouette score for k(clusters) = 8 is 0.4592958445675391\nSilhouette score for k(clusters) = 9 is 0.45770857148861777\nSilhouette score for k(clusters) = 10 is 0.446735677440187\n\n\n\n\n\n\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(X)\n\n\n\n\n\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\n\n\n\n\n\nkmeans.cluster_centers_\n\narray([[55.2962963 , 49.51851852],\n       [88.2       , 17.11428571],\n       [26.30434783, 20.91304348],\n       [25.72727273, 79.36363636],\n       [86.53846154, 82.12820513]])\n\n\n\nkmeans.labels_\n\narray([2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n       2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0,\n       2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 0, 4, 1, 4, 1, 4,\n       0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n       1, 4], dtype=int32)\n\n\n\n\n\n\n\n\n\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()\n\n\n\n\n\nhelp(sch.linkage)\n\nHelp on function linkage in module scipy.cluster.hierarchy:\n\nlinkage(y, method='single', metric='euclidean', optimal_ordering=False)\n    Perform hierarchical/agglomerative clustering.\n    \n    The input y may be either a 1-D condensed distance matrix\n    or a 2-D array of observation vectors.\n    \n    If y is a 1-D condensed distance matrix,\n    then y must be a :math:`\\binom{n}{2}` sized\n    vector, where n is the number of original observations paired\n    in the distance matrix. The behavior of this function is very\n    similar to the MATLAB linkage function.\n    \n    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the\n    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and\n    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A\n    cluster with an index less than :math:`n` corresponds to one of\n    the :math:`n` original observations. The distance between\n    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The\n    fourth value ``Z[i, 3]`` represents the number of original\n    observations in the newly formed cluster.\n    \n    The following linkage methods are used to compute the distance\n    :math:`d(s, t)` between two clusters :math:`s` and\n    :math:`t`. The algorithm begins with a forest of clusters that\n    have yet to be used in the hierarchy being formed. When two\n    clusters :math:`s` and :math:`t` from this forest are combined\n    into a single cluster :math:`u`, :math:`s` and :math:`t` are\n    removed from the forest, and :math:`u` is added to the\n    forest. When only one cluster remains in the forest, the algorithm\n    stops, and this cluster becomes the root.\n    \n    A distance matrix is maintained at each iteration. The ``d[i,j]``\n    entry corresponds to the distance between cluster :math:`i` and\n    :math:`j` in the original forest.\n    \n    At each iteration, the algorithm must update the distance matrix\n    to reflect the distance of the newly formed cluster u with the\n    remaining clusters in the forest.\n    \n    Suppose there are :math:`|u|` original observations\n    :math:`u[0], \\ldots, u[|u|-1]` in cluster :math:`u` and\n    :math:`|v|` original objects :math:`v[0], \\ldots, v[|v|-1]` in\n    cluster :math:`v`. Recall, :math:`s` and :math:`t` are\n    combined to form cluster :math:`u`. Let :math:`v` be any\n    remaining cluster in the forest that is not :math:`u`.\n    \n    The following are methods for calculating the distance between the\n    newly formed cluster :math:`u` and each :math:`v`.\n    \n      * method='single' assigns\n    \n        .. math::\n           d(u,v) = \\min(dist(u[i],v[j]))\n    \n        for all points :math:`i` in cluster :math:`u` and\n        :math:`j` in cluster :math:`v`. This is also known as the\n        Nearest Point Algorithm.\n    \n      * method='complete' assigns\n    \n        .. math::\n           d(u, v) = \\max(dist(u[i],v[j]))\n    \n        for all points :math:`i` in cluster u and :math:`j` in\n        cluster :math:`v`. This is also known by the Farthest Point\n        Algorithm or Voor Hees Algorithm.\n    \n      * method='average' assigns\n    \n        .. math::\n           d(u,v) = \\sum_{ij} \\frac{d(u[i], v[j])}\n                                   {(|u|*|v|)}\n    \n        for all points :math:`i` and :math:`j` where :math:`|u|`\n        and :math:`|v|` are the cardinalities of clusters :math:`u`\n        and :math:`v`, respectively. This is also called the UPGMA\n        algorithm.\n    \n      * method='weighted' assigns\n    \n        .. math::\n           d(u,v) = (dist(s,v) + dist(t,v))/2\n    \n        where cluster u was formed with cluster s and t and v\n        is a remaining cluster in the forest (also called WPGMA).\n    \n      * method='centroid' assigns\n    \n        .. math::\n           dist(s,t) = ||c_s-c_t||_2\n    \n        where :math:`c_s` and :math:`c_t` are the centroids of\n        clusters :math:`s` and :math:`t`, respectively. When two\n        clusters :math:`s` and :math:`t` are combined into a new\n        cluster :math:`u`, the new centroid is computed over all the\n        original objects in clusters :math:`s` and :math:`t`. The\n        distance then becomes the Euclidean distance between the\n        centroid of :math:`u` and the centroid of a remaining cluster\n        :math:`v` in the forest. This is also known as the UPGMC\n        algorithm.\n    \n      * method='median' assigns :math:`d(s,t)` like the ``centroid``\n        method. When two clusters :math:`s` and :math:`t` are combined\n        into a new cluster :math:`u`, the average of centroids s and t\n        give the new centroid :math:`u`. This is also known as the\n        WPGMC algorithm.\n    \n      * method='ward' uses the Ward variance minimization algorithm.\n        The new entry :math:`d(u,v)` is computed as follows,\n    \n        .. math::\n    \n           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\frac{|v|}\n                               {T}d(s,t)^2}\n    \n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    \n    Warning: When the minimum distance pair in the forest is chosen, there\n    may be two or more pairs with the same minimum distance. This\n    implementation may choose a different minimum than the MATLAB\n    version.\n    \n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed distance matrix\n        is a flat array containing the upper triangular of the distance matrix.\n        This is the form that ``pdist`` returns. Alternatively, a collection of\n        :math:`m` observation vectors in :math:`n` dimensions may be passed as\n        an :math:`m` by :math:`n` array. All elements of the condensed distance\n        matrix must be finite, i.e., no NaNs or infs.\n    method : str, optional\n        The linkage algorithm to use. See the ``Linkage Methods`` section below\n        for full descriptions.\n    metric : str or function, optional\n        The distance metric to use in the case that y is a collection of\n        observation vectors; ignored otherwise. See the ``pdist``\n        function for a list of valid distance metrics. A custom distance\n        function can also be used.\n    optimal_ordering : bool, optional\n        If True, the linkage matrix will be reordered so that the distance\n        between successive leaves is minimal. This results in a more intuitive\n        tree structure when the data are visualized. defaults to False, because\n        this algorithm can be slow, particularly on large datasets [2]_. See\n        also the `optimal_leaf_ordering` function.\n    \n        .. versionadded:: 1.0.0\n    \n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix.\n    \n    Notes\n    -----\n    1. For method 'single', an optimized algorithm based on minimum spanning\n       tree is implemented. It has time complexity :math:`O(n^2)`.\n       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm\n       called nearest-neighbors chain is implemented. It also has time\n       complexity :math:`O(n^2)`.\n       For other methods, a naive algorithm is implemented with :math:`O(n^3)`\n       time complexity.\n       All algorithms use :math:`O(n^2)` memory.\n       Refer to [1]_ for details about the algorithms.\n    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if\n       Euclidean pairwise metric is used. If `y` is passed as precomputed\n       pairwise distances, then it is the user's responsibility to assure that\n       these distances are in fact Euclidean, otherwise the produced result\n       will be incorrect.\n    \n    See Also\n    --------\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    \n    References\n    ----------\n    .. [1] Daniel Mullner, \"Modern hierarchical, agglomerative clustering\n           algorithms\", :arXiv:`1109.2378v1`.\n    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, \"Fast optimal\n           leaf ordering for hierarchical clustering\", 2001. Bioinformatics\n           :doi:`10.1093/bioinformatics/17.suppl_1.S22`\n    \n    Examples\n    --------\n    >>> from scipy.cluster.hierarchy import dendrogram, linkage\n    >>> from matplotlib import pyplot as plt\n    >>> X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n    \n    >>> Z = linkage(X, 'ward')\n    >>> fig = plt.figure(figsize=(25, 10))\n    >>> dn = dendrogram(Z)\n    \n    >>> Z = linkage(X, 'single')\n    >>> fig = plt.figure(figsize=(25, 10))\n    >>> dn = dendrogram(Z)\n    >>> plt.show()\n\n\n\n\n\n\nHierarchical clustering adalah keluarga umum dari algoritma clustering yang membangun cluster bersarang dengan menggabungkan atau memisahkannya secara berurutan. Hirarki cluster ini direpresentasikan sebagai sebuah pohon (atau dendogram). Akar dari pohon adalah cluster unik yang mengumpulkan semua sampel, sedangkan daunnya adalah cluster yang hanya memiliki satu sampel.\nObjek AgglomerativeClustering melakukan pengelompokan hirarkis menggunakan pendekatan dari bawah ke atas: setiap pengamatan dimulai dari klasternya sendiri, dan klaster-klaster tersebut digabungkan secara berurutan. Kriteria keterkaitan menentukan metrik yang digunakan untuk strategi penggabungan:\n\nWard meminimalkan jumlah perbedaan kuadrat di dalam semua cluster. Ini adalah pendekatan yang meminimalkan varians dan dalam hal ini mirip dengan fungsi objektif k-means tetapi ditangani dengan pendekatan hirarki aglomeratif.\nMaximum atau complete linkage meminimalkan jarak maksimum antara pengamatan dari pasangan cluster.\nAverage linkage meminimalkan rata-rata jarak antara semua pengamatan dari pasangan cluster.\nSingle linkage meminimalkan jarak antara pengamatan terdekat dari pasangan cluster.\n\nAgglomerativeClustering juga dapat menskalakan ke sejumlah besar sampel ketika digunakan bersama dengan matriks konektivitas, tetapi secara komputasi mahal ketika tidak ada batasan konektivitas yang ditambahkan di antara sampel: ia mempertimbangkan pada setiap langkah semua kemungkinan penggabungan.\nsource : https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering\n\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)\n\n\n\n\n\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "main-module/week-05.html",
    "href": "main-module/week-05.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "# TODO\n- [ ] keras explanation\n- [ ] tambahin kata-kata penjelas"
  },
  {
    "objectID": "main-module/week-05.html#prerequisites",
    "href": "main-module/week-05.html#prerequisites",
    "title": "sains-data-2023",
    "section": "prerequisites",
    "text": "prerequisites\n\n# !pip install tensorflow # uncomment if you don't have tensorflow installed\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np"
  },
  {
    "objectID": "main-module/week-05.html#all-about-tensors-and-tensorflow",
    "href": "main-module/week-05.html#all-about-tensors-and-tensorflow",
    "title": "sains-data-2023",
    "section": "All about Tensors and Tensorflow",
    "text": "All about Tensors and Tensorflow\n\n# All-ones or all-zeros tensors\n\nx = tf.ones(shape = (2,1)) # 2x3 matrix of ones, similar to np.ones((2,1))\nprint(x)\n\nx = tf.zeros(shape = (2,1)) # 2x3 matrix of zeros, similar to np.zeros((2,1))\nprint(x)\n\n\ntf.Tensor(\n[[1.]\n [1.]], shape=(2, 1), dtype=float32)\ntf.Tensor(\n[[0.]\n [0.]], shape=(2, 1), dtype=float32)\n\n\n\nx.__class__\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n# Random tensors\n\n# create a tensor with random values from a normal distribution\nx = tf.random.normal(shape = (2,3), mean = 0, stddev = 1)\nprint(x)\n\n# create a tensor with random values from a uniform distribution\nx = tf.random.uniform(shape = (2,3), minval = 0, maxval = 1)\nprint(x)\n\ntf.Tensor(\n[[ 1.1509199  -1.4267175  -0.48263487]\n [-0.30107084  0.90581864  1.0413685 ]], shape=(2, 3), dtype=float32)\ntf.Tensor(\n[[0.6582366  0.34180927 0.91947365]\n [0.5607337  0.9532844  0.31796992]], shape=(2, 3), dtype=float32)\n\n\n\n# numpy array are assignable while tensors are not\nx = np.random.normal(loc = 0, scale = 1, size = (2,3))\nx[0,0] = 100\nprint(x)\n\n[[ 1.00000000e+02 -1.25304057e+00 -1.18967720e+00]\n [ 4.74877369e-01 -8.13430401e-02 -4.57822064e-01]]\n\n\n\n# numpy array are assignable while tensors are not\nx = tf.ones(shape = (2,3))\nx[0,0] = 100\nprint(x)\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n\n# Creating a TensorFlow variable\nv = tf.Variable(initial_value = tf.random.normal(shape = (2,3)))\nprint(v)\nprint()\n\nv.assign(tf.zeros(shape = (2,3)))\nprint(v)\n\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[-0.10799041,  2.325188  , -0.20042379],\n       [ 0.48759696,  0.53195345,  0.29525948]], dtype=float32)>\n\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>\n\n\n\n# Assigning a value to a subset of a TensorFlow variable\nv[0,0].assign(100)\nprint(v)\n\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[100.,   0.,   0.],\n       [  0.,   0.,   0.]], dtype=float32)>\n\n\n\n# Assign add\nv.assign_add(tf.ones(shape = (2,3)))\nprint(v)\n\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[101.,   1.,   1.],\n       [  1.,   1.,   1.]], dtype=float32)>\n\n\n\n# just like numpy, TensorFlow offers a large collection of tensor operations to express\n# mathematical formulas.\na = tf.ones((2, 2))\nb = tf.square(a)\nc = tf.sqrt(a)\nd = b + c\ne = tf.matmul(a, b)\ne *= d\nprint(e)\n\ntf.Tensor(\n[[4. 4.]\n [4. 4.]], shape=(2, 2), dtype=float32)\n\n\nSo far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy can’t do: retrieve the gradient of any differentiable expression with respect to any of its inputs. Just open a GradientTape scope, apply some computation to one or several input tensors, and retrieve the gradient of the result with respect to the inputs\n\n# Using the GradientTape\ninput_var = tf.Variable(initial_value = 3.0)\nwith tf.GradientTape() as tape:\n    result = tf.square(input_var)\ngrad = tape.gradient(result, input_var)\nprint(grad)\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\n\n# Using GradientTape with constant tensor inputs\ninput_var = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n    tape.watch(input_var)\n    result = tf.square(input_var)\ngrad = tape.gradient(result, input_var)\nprint(grad)\n\ntf.Tensor(6.0, shape=(), dtype=float32)\n\n\n\n# Using nested gradient tapes to compute second-order gradients\ntime = tf.Variable(0.0)\nwith tf.GradientTape() as outer_tape:\n    with tf.GradientTape() as inner_tape:\n        position = 4.9 * time ** 2\n    speed = inner_tape.gradient(position, time) \nacceleration = outer_tape.gradient(speed, time)\n\nprint(speed)\nprint(acceleration)\n\n\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(9.8, shape=(), dtype=float32)"
  },
  {
    "objectID": "main-module/week-05.html#an-end-to-end-example-a-linear-classifier-in-pure-tensorflow",
    "href": "main-module/week-05.html#an-end-to-end-example-a-linear-classifier-in-pure-tensorflow",
    "title": "sains-data-2023",
    "section": "An end-to-end example: A linear classifier in pure TensorFlow",
    "text": "An end-to-end example: A linear classifier in pure TensorFlow\n\n# Generating two classes of random points in a 2D plane\nnum_samples_per_class, num_classes = 1000, 2\nnegative_samples = np.random.multivariate_normal(mean = [0,3], cov = [[1,0.5],[0.5,1]], size = num_samples_per_class)\npositive_samples = np.random.multivariate_normal(mean = [3,0], cov = [[1,0.5],[0.5,1]], size = num_samples_per_class)\n\ninputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\ntargets = np.vstack((np.zeros((num_samples_per_class, 1), dtype = 'float32'), np.ones((num_samples_per_class, 1), dtype = 'float32')))\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])\nplt.show()\n\n\n\n\n\n# Creating the linear classifier variables\ninput_dim = 2\noutput_dim = 1\nW = tf.Variable(tf.random.normal(shape = (input_dim, output_dim)))\nb = tf.Variable(tf.random.normal(shape = (output_dim,)))\n\n\n\n# the forward pass\ndef model(inputs):\n    return tf.sigmoid(tf.matmul(inputs, W) + b)\n    \n# The mean squared error loss function\n\ndef entropy_loss(targets, predictions):\n    per_sample_losses = - targets * tf.math.log(predictions) - (1 - targets) * tf.math.log(1 - predictions)\n    return tf.reduce_mean(per_sample_losses)\n\n\n# training step \nlearning_rate = 0.1\ndef training_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        loss = square_loss(targets, predictions)\n        grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n        W.assign_sub(learning_rate * grad_loss_wrt_W)\n        b.assign_sub(learning_rate * grad_loss_wrt_b)\n        return loss\n\n\n\n\n\n# training loop/process/epoch\nfor step in range(100):\n    loss = training_step(inputs, targets)\n    print(f\"Loss at step {step}: {loss:.4f}\")\n\nLoss at step 0: 0.0495\nLoss at step 1: 0.0473\nLoss at step 2: 0.0454\nLoss at step 3: 0.0436\nLoss at step 4: 0.0420\nLoss at step 5: 0.0406\nLoss at step 6: 0.0392\nLoss at step 7: 0.0380\nLoss at step 8: 0.0369\nLoss at step 9: 0.0358\nLoss at step 10: 0.0348\nLoss at step 11: 0.0339\nLoss at step 12: 0.0330\nLoss at step 13: 0.0322\nLoss at step 14: 0.0315\nLoss at step 15: 0.0308\nLoss at step 16: 0.0301\nLoss at step 17: 0.0295\nLoss at step 18: 0.0289\nLoss at step 19: 0.0283\nLoss at step 20: 0.0278\nLoss at step 21: 0.0273\nLoss at step 22: 0.0268\nLoss at step 23: 0.0263\nLoss at step 24: 0.0259\nLoss at step 25: 0.0255\nLoss at step 26: 0.0251\nLoss at step 27: 0.0247\nLoss at step 28: 0.0243\nLoss at step 29: 0.0240\nLoss at step 30: 0.0236\nLoss at step 31: 0.0233\nLoss at step 32: 0.0230\nLoss at step 33: 0.0227\nLoss at step 34: 0.0224\nLoss at step 35: 0.0221\nLoss at step 36: 0.0218\nLoss at step 37: 0.0215\nLoss at step 38: 0.0213\nLoss at step 39: 0.0210\nLoss at step 40: 0.0208\nLoss at step 41: 0.0205\nLoss at step 42: 0.0203\nLoss at step 43: 0.0201\nLoss at step 44: 0.0198\nLoss at step 45: 0.0196\nLoss at step 46: 0.0194\nLoss at step 47: 0.0192\nLoss at step 48: 0.0190\nLoss at step 49: 0.0188\nLoss at step 50: 0.0186\nLoss at step 51: 0.0185\nLoss at step 52: 0.0183\nLoss at step 53: 0.0181\nLoss at step 54: 0.0179\nLoss at step 55: 0.0178\nLoss at step 56: 0.0176\nLoss at step 57: 0.0174\nLoss at step 58: 0.0173\nLoss at step 59: 0.0171\nLoss at step 60: 0.0170\nLoss at step 61: 0.0168\nLoss at step 62: 0.0167\nLoss at step 63: 0.0166\nLoss at step 64: 0.0164\nLoss at step 65: 0.0163\nLoss at step 66: 0.0162\nLoss at step 67: 0.0160\nLoss at step 68: 0.0159\nLoss at step 69: 0.0158\nLoss at step 70: 0.0157\nLoss at step 71: 0.0155\nLoss at step 72: 0.0154\nLoss at step 73: 0.0153\nLoss at step 74: 0.0152\nLoss at step 75: 0.0151\nLoss at step 76: 0.0150\nLoss at step 77: 0.0149\nLoss at step 78: 0.0148\nLoss at step 79: 0.0147\nLoss at step 80: 0.0146\nLoss at step 81: 0.0145\nLoss at step 82: 0.0144\nLoss at step 83: 0.0143\nLoss at step 84: 0.0142\nLoss at step 85: 0.0141\nLoss at step 86: 0.0140\nLoss at step 87: 0.0139\nLoss at step 88: 0.0138\nLoss at step 89: 0.0137\nLoss at step 90: 0.0136\nLoss at step 91: 0.0135\nLoss at step 92: 0.0135\nLoss at step 93: 0.0134\nLoss at step 94: 0.0133\nLoss at step 95: 0.0132\nLoss at step 96: 0.0131\nLoss at step 97: 0.0131\nLoss at step 98: 0.0130\nLoss at step 99: 0.0129\n\n\n\npredictions = model(inputs)\nprint(predictions)\nplt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\nplt.show()\n\ntf.Tensor(\n[[0.04117302]\n [0.02456259]\n [0.00931301]\n ...\n [0.9823857 ]\n [0.9144001 ]\n [0.98359877]], shape=(2000, 1), dtype=float32)"
  },
  {
    "objectID": "tugas/tugas-2.html",
    "href": "tugas/tugas-2.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Kerjakan secara individu\nKerjakan tugas ini dengan bahasa pemrograman python. Anda disarankan menggunakan jupyter untuk mengerjakan tugas ini.\nUntuk setiap proses sains data (pembersihan data, transformasi data, EDA, dan pemodela ) yang dilakukan Anda diperlukan untuk menuliskan justifikasi-nya. Justifikasi-nya dapat berupa penjelasan singkat mengenai proses yang dilakukan, dan penjelasan mengenai alasan mengapa anda melakukan proses tersebut.\nFile yang harus diunggah terdiri dari:\n\nbeberapa model dalam format .pkl. Penamaan untuk model dibebaskan, namun harus jelas mengenai model apa yang disimpan.\nsatu file python notebook (file berbentuk .ipynb BUKAN .py) dengan ketentuan serupa.\n\nSemua file disatukan dalam 1 (satu) file .zip, dengan format penamaan: Nama_NPM_Kelas SIAK Sains Data_Tugas2PrakSainsData.zip. contoh: Itadori-Yuji_190688675_A_Tugas2PrakSainsData.zip\nBatas pengumpulan tugas ini adalah 21 April 2023 pukul 23.59. Tugas dikumpulkan sesuai dengan link berikut: https://ristek.link/tugas-sains-data-02\nDilarang melakukan plagiarisme atau menduplikasi dalam mengerjakan tugas ini. Apabila terdapat kesamaan program atau penjelasan pada tugas yang dikumpulkan, NILAI TUGAS PRAKTIKUM SAINS DATA ANDA LANGSUNG MENJADI 0 TANPA PERINGATAN bagi semua pihak yang terlibat plagiarisme dalam tugas ini.\nGunakan module (python package) yang telah dipelajari di praktikum atau kelas. Anda diperbolehkan untuk menggunakan module (python package) lain dengan catatan bahwa Anda harus menuliskan penjelasan singkat mengenai module tersebut.\nApabila ada yang ingin ditanyakan anda dapat bertanya pada kolom komentar atau, silakan mengontak salah satu kontak berikut:\n\nLINE: Carles_Octavianus (carles)\n\n\n\n\n\n[akses-data]: https://drive.google.com/open?id=1whKzd5rd-Rtg8bGmEYeBonsXLQMKfxTB&authuser=carlesoctavianus%40gmail.com&usp=drive_fs\nKerjakan secara end-to-end (pembersihan data, transformasi data , EDA, dan pemodelan) untuk memprediksi harga rumah berdasarkan data yang diberikan. Gunakan metode yang telah dipelajari di praktikum ataupun kelas (model regresi)."
  },
  {
    "objectID": "tugas/tugas-3.html",
    "href": "tugas/tugas-3.html",
    "title": "sains-data-2023",
    "section": "",
    "text": "Kerjakan secara individu\nKerjakan tugas ini dengan bahasa pemrograman python. Anda disarankan menggunakan jupyter untuk mengerjakan tugas ini.\nUntuk setiap proses sains data (pembersihan data, transformasi data, EDA, dan pemodela ) yang dilakukan Anda diperlukan untuk menuliskan justifikasi-nya. Justifikasi-nya dapat berupa penjelasan singkat mengenai proses yang dilakukan, dan penjelasan mengenai alasan mengapa anda melakukan proses tersebut.\nFile yang harus diunggah terdiri dari:\n\nbeberapa model dalam format .pkl. Penamaan untuk model dibebaskan, namun harus jelas mengenai model apa yang disimpan.\nsatu file python notebook (file berbentuk .ipynb BUKAN .py) dengan ketentuan serupa.\n\nSemua file disatukan dalam 1 (satu) file .zip, dengan format penamaan: Nama_NPM_Kelas SIAK Sains Data_Tugas3PrakSainsData.zip. contoh: Itadori-Yuji_190688675_A_Tugas3PrakSainsData.zip\nBatas pengumpulan tugas ini adalah 21 April 2023 pukul 23.59. Tugas dikumpulkan sesuai dengan link berikut: https://ristek.link/tugas-sains-data-03\nDilarang melakukan plagiarisme atau menduplikasi dalam mengerjakan tugas ini. Apabila terdapat kesamaan program atau penjelasan pada tugas yang dikumpulkan, NILAI TUGAS PRAKTIKUM SAINS DATA ANDA LANGSUNG MENJADI 0 TANPA PERINGATAN bagi semua pihak yang terlibat plagiarisme dalam tugas ini.\nGunakan module (python package) yang telah dipelajari di praktikum atau kelas. Anda diperbolehkan untuk menggunakan module (python package) lain dengan catatan bahwa Anda harus menuliskan penjelasan singkat mengenai module tersebut.\nApabila ada yang ingin ditanyakan anda dapat bertanya pada kolom komentar atau, silakan mengontak salah satu kontak berikut:\n\nLINE: Tulus Setiawan (WA/LINE: tlsnew/081213679316)\n\n\n\n\n\n[akses-data]: https://drive.google.com/open?id=19WogXg2YgH7tNhAXESJ7SaITOWGGu2HX&authuser=carlesoctavianus%40gmail.com&usp=drive_fs\nKerjakan secara end-to-end (pembersihan data, transformasi data , EDA, dan pemodelan) untuk mengklasifikasikan harga ponsel berdasarkan data yang diberikan. Gunakan metode yang telah dipelajari di praktikum ataupun kelas (model Klasifikasi)."
  }
]